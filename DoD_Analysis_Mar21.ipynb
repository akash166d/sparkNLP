{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DoD_Analysis_Mar21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMsw7c8r6NwgvBST5yvq/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash166d/sparkNLP/blob/master/DoD_Analysis_Mar21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSs1M2BVAn6Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBmc0480QLzB"
      },
      "source": [
        "# Install Package/Lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwc2OqTH16VJ"
      },
      "source": [
        "This is required to ensure the ubuntu related dependencies are up to date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQwcI3O3KCty",
        "outputId": "68600f7f-7774-4995-b964-e1e18e408e4b"
      },
      "source": [
        "! sudo apt-get update --fix-missing\n",
        "! sudo apt-get upgrade --fix-missing"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Wait\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [2 In\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [602 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,964 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,163 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,745 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,396 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,394 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [893 kB]\n",
            "Fetched 11.4 MB in 4s (3,265 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  binutils binutils-common binutils-x86-64-linux-gnu cuda-compat-11-0 git\n",
            "  git-man gnupg2 libaudit-common libaudit1 libbinutils libc-bin libcublas-dev\n",
            "  libcublas10 libcudnn7 libcudnn7-dev libglib2.0-0 libglib2.0-bin\n",
            "  libglib2.0-data libglib2.0-dev libglib2.0-dev-bin libldap-2.4-2\n",
            "  libldap-common libp11-kit0 libperl5.26 libsasl2-2 libsasl2-modules-db\n",
            "  libzstd1 linux-libc-dev nsight-compute-2020.3.1 openssl perl perl-base\n",
            "  perl-modules-5.26 r-cran-desc r-cran-dplyr r-cran-isoband r-cran-pillar\n",
            "  r-cran-rvest r-cran-tinytex r-cran-waldo tar\n",
            "41 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 745 MB of archives.\n",
            "After this operation, 68.9 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-compat-11-0 450.102.04-1 [6,712 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas10 10.2.3.254-1 [43.1 MB]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-desc all 1.3.0-1cran1.1804.0 [493 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libperl5.26 amd64 5.26.1-6ubuntu0.5 [3,534 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas-dev 10.2.3.254-1 [42.4 MB]\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-dplyr amd64 1.0.5-1cran1.1804.0 [1,054 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7-dev 7.6.5.32-1+cuda10.2 [165 MB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-isoband amd64 0.2.4-1cran1.1804.0 [2,096 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl amd64 5.26.1-6ubuntu0.5 [201 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl-base amd64 5.26.1-6ubuntu0.5 [1,391 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl-modules-5.26 all 5.26.1-6ubuntu0.5 [2,762 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 tar amd64 1.29b-2ubuntu0.2 [234 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-bin amd64 2.27-3ubuntu1.4 [643 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit-common all 1:2.8.2-1ubuntu1.1 [4,068 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit1 amd64 1:2.8.2-1ubuntu1.1 [38.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzstd1 amd64 1.3.3+dfsg-2ubuntu1.2 [189 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libp11-kit0 amd64 0.23.9-2ubuntu0.1 [187 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-dev amd64 2.56.4-0ubuntu0.18.04.7 [1,386 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-dev-bin amd64 2.56.4-0ubuntu0.18.04.7 [102 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-data all 2.56.4-0ubuntu0.18.04.7 [4,600 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-bin amd64 2.56.4-0ubuntu0.18.04.7 [68.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-0 amd64 2.56.4-0ubuntu0.18.04.7 [1,172 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssl amd64 1.1.1-1ubuntu2.1~18.04.8 [614 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.5 [1,839 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.5 [197 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-pillar all 1.5.1-1cran1.1804.0 [832 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.5 [3,388 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.5 [489 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 git-man all 1:2.17.1-1ubuntu0.8 [804 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 git amd64 1:2.17.1-1ubuntu0.8 [3,916 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [15.0 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [49.2 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.10 [15.8 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.10 [154 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-136.140 [1,001 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 gnupg2 all 2.2.4-1ubuntu1.4 [5,292 B]\n",
            "Get:37 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-rvest all 1.0.0-1cran1.1804.0 [191 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7 7.6.5.32-1+cuda10.2 [189 MB]\n",
            "Get:39 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-tinytex all 0.30-1cran1.1804.0 [117 kB]\n",
            "Get:40 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-waldo all 0.2.5-1cran1.1804.0 [72.0 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nsight-compute-2020.3.1 2020.3.1.4-1 [273 MB]\n",
            "Fetched 745 MB in 14s (54.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 41.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libperl5.26_5.26.1-6ubuntu0.5_amd64.deb ...\n",
            "Unpacking libperl5.26:amd64 (5.26.1-6ubuntu0.5) over (5.26.1-6ubuntu0.3) ...\n",
            "Preparing to unpack .../perl_5.26.1-6ubuntu0.5_amd64.deb ...\n",
            "Unpacking perl (5.26.1-6ubuntu0.5) over (5.26.1-6ubuntu0.3) ...\n",
            "Preparing to unpack .../perl-base_5.26.1-6ubuntu0.5_amd64.deb ...\n",
            "Unpacking perl-base (5.26.1-6ubuntu0.5) over (5.26.1-6ubuntu0.3) ...\n",
            "Setting up perl-base (5.26.1-6ubuntu0.5) ...\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../perl-modules-5.26_5.26.1-6ubuntu0.5_all.deb ...\n",
            "Unpacking perl-modules-5.26 (5.26.1-6ubuntu0.5) over (5.26.1-6ubuntu0.3) ...\n",
            "Preparing to unpack .../tar_1.29b-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking tar (1.29b-2ubuntu0.2) over (1.29b-2ubuntu0.1) ...\n",
            "Setting up tar (1.29b-2ubuntu0.2) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/sbin/rmt-tar because link group rmt is broken\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-bin_2.27-3ubuntu1.4_amd64.deb ...\n",
            "Unpacking libc-bin (2.27-3ubuntu1.4) over (2.27-3ubuntu1.2) ...\n",
            "Setting up libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libaudit-common_1%3a2.8.2-1ubuntu1.1_all.deb ...\n",
            "Unpacking libaudit-common (1:2.8.2-1ubuntu1.1) over (1:2.8.2-1ubuntu1) ...\n",
            "Setting up libaudit-common (1:2.8.2-1ubuntu1.1) ...\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libaudit1_1%3a2.8.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libaudit1:amd64 (1:2.8.2-1ubuntu1.1) over (1:2.8.2-1ubuntu1) ...\n",
            "Setting up libaudit1:amd64 (1:2.8.2-1ubuntu1.1) ...\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libzstd1_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking libzstd1:amd64 (1.3.3+dfsg-2ubuntu1.2) over (1.3.3+dfsg-2ubuntu1.1) ...\n",
            "Setting up libzstd1:amd64 (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libp11-kit0_0.23.9-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libp11-kit0:amd64 (0.23.9-2ubuntu0.1) over (0.23.9-2) ...\n",
            "Setting up libp11-kit0:amd64 (0.23.9-2ubuntu0.1) ...\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglib2.0-dev_2.56.4-0ubuntu0.18.04.7_amd64.deb ...\n",
            "Unpacking libglib2.0-dev:amd64 (2.56.4-0ubuntu0.18.04.7) over (2.56.4-0ubuntu0.18.04.6) ...\n",
            "Preparing to unpack .../01-libglib2.0-dev-bin_2.56.4-0ubuntu0.18.04.7_amd64.deb ...\n",
            "Unpacking libglib2.0-dev-bin (2.56.4-0ubuntu0.18.04.7) over (2.56.4-0ubuntu0.18.04.6) ...\n",
            "Preparing to unpack .../02-libglib2.0-data_2.56.4-0ubuntu0.18.04.7_all.deb ...\n",
            "Unpacking libglib2.0-data (2.56.4-0ubuntu0.18.04.7) over (2.56.4-0ubuntu0.18.04.6) ...\n",
            "Preparing to unpack .../03-libglib2.0-bin_2.56.4-0ubuntu0.18.04.7_amd64.deb ...\n",
            "Unpacking libglib2.0-bin (2.56.4-0ubuntu0.18.04.7) over (2.56.4-0ubuntu0.18.04.6) ...\n",
            "Preparing to unpack .../04-libglib2.0-0_2.56.4-0ubuntu0.18.04.7_amd64.deb ...\n",
            "Unpacking libglib2.0-0:amd64 (2.56.4-0ubuntu0.18.04.7) over (2.56.4-0ubuntu0.18.04.6) ...\n",
            "Preparing to unpack .../05-openssl_1.1.1-1ubuntu2.1~18.04.8_amd64.deb ...\n",
            "Unpacking openssl (1.1.1-1ubuntu2.1~18.04.8) over (1.1.1-1ubuntu2.1~18.04.6) ...\n",
            "Preparing to unpack .../06-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../07-binutils-common_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../08-binutils_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking binutils (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../09-libbinutils_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../10-cuda-compat-11-0_450.102.04-1_amd64.deb ...\n",
            "Unpacking cuda-compat-11-0 (450.102.04-1) over (450.80.02-1) ...\n",
            "Preparing to unpack .../11-git-man_1%3a2.17.1-1ubuntu0.8_all.deb ...\n",
            "Unpacking git-man (1:2.17.1-1ubuntu0.8) over (1:2.17.1-1ubuntu0.7) ...\n",
            "Preparing to unpack .../12-git_1%3a2.17.1-1ubuntu0.8_amd64.deb ...\n",
            "Unpacking git (1:2.17.1-1ubuntu0.8) over (1:2.17.1-1ubuntu0.7) ...\n",
            "Preparing to unpack .../13-libcublas10_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas10 (10.2.3.254-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../14-libcublas-dev_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas-dev (10.2.3.254-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../15-libcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in auto mode\n",
            "Unpacking libcudnn7-dev (7.6.5.32-1+cuda10.2) over (7.6.5.32-1+cuda10.1) ...\n",
            "Preparing to unpack .../16-libcudnn7_7.6.5.32-1+cuda10.2_amd64.deb ...\n",
            "Unpacking libcudnn7 (7.6.5.32-1+cuda10.2) over (7.6.5.32-1+cuda10.1) ...\n",
            "Preparing to unpack .../17-libsasl2-modules-db_2.1.27~101-g0780600+dfsg-3ubuntu2.3_amd64.deb ...\n",
            "Unpacking libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) over (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Preparing to unpack .../18-libsasl2-2_2.1.27~101-g0780600+dfsg-3ubuntu2.3_amd64.deb ...\n",
            "Unpacking libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) over (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Preparing to unpack .../19-libldap-common_2.4.45+dfsg-1ubuntu1.10_all.deb ...\n",
            "Unpacking libldap-common (2.4.45+dfsg-1ubuntu1.10) over (2.4.45+dfsg-1ubuntu1.6) ...\n",
            "Preparing to unpack .../20-libldap-2.4-2_2.4.45+dfsg-1ubuntu1.10_amd64.deb ...\n",
            "Unpacking libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.10) over (2.4.45+dfsg-1ubuntu1.6) ...\n",
            "Preparing to unpack .../21-linux-libc-dev_4.15.0-136.140_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-136.140) over (4.15.0-118.119) ...\n",
            "Preparing to unpack .../22-nsight-compute-2020.3.1_2020.3.1.4-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2020.3.1 (2020.3.1.4-1) over (2020.3.1.3-1) ...\n",
            "Preparing to unpack .../23-r-cran-desc_1.3.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-desc (1.3.0-1cran1.1804.0) over (1.2.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../24-r-cran-dplyr_1.0.5-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-dplyr (1.0.5-1cran1.1804.0) over (1.0.4-1cran1.1804.0) ...\n",
            "Preparing to unpack .../25-r-cran-isoband_0.2.4-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-isoband (0.2.4-1cran1.1804.0) over (0.2.3-1cran1.1804.0) ...\n",
            "Preparing to unpack .../26-r-cran-pillar_1.5.1-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-pillar (1.5.1-1cran1.1804.0) over (1.5.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../27-r-cran-rvest_1.0.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-rvest (1.0.0-1cran1.1804.0) over (0.3.6-1cran1.1804.0) ...\n",
            "Preparing to unpack .../28-r-cran-tinytex_0.30-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-tinytex (0.30-1cran1.1804.0) over (0.29-1cran1.1804.0) ...\n",
            "Preparing to unpack .../29-r-cran-waldo_0.2.5-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-waldo (0.2.5-1cran1.1804.0) over (0.2.4-1cran1.1804.0) ...\n",
            "Preparing to unpack .../30-gnupg2_2.2.4-1ubuntu1.4_all.deb ...\n",
            "Unpacking gnupg2 (2.2.4-1ubuntu1.4) over (2.2.4-1ubuntu1.3) ...\n",
            "Setting up r-cran-tinytex (0.30-1cran1.1804.0) ...\n",
            "Setting up libcudnn7 (7.6.5.32-1+cuda10.2) ...\n",
            "Setting up git-man (1:2.17.1-1ubuntu0.8) ...\n",
            "Setting up libldap-common (2.4.45+dfsg-1ubuntu1.10) ...\n",
            "Setting up cuda-compat-11-0 (450.102.04-1) ...\n",
            "Setting up nsight-compute-2020.3.1 (2020.3.1.4-1) ...\n",
            "Setting up libglib2.0-0:amd64 (2.56.4-0ubuntu0.18.04.7) ...\n",
            "Setting up libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) ...\n",
            "Setting up r-cran-desc (1.3.0-1cran1.1804.0) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-136.140) ...\n",
            "Setting up libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) ...\n",
            "Setting up libcudnn7-dev (7.6.5.32-1+cuda10.2) ...\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v7.h to provide /usr/include/cudnn.h (libcudnn) in manual mode\n",
            "Setting up perl-modules-5.26 (5.26.1-6ubuntu0.5) ...\n",
            "Setting up r-cran-isoband (0.2.4-1cran1.1804.0) ...\n",
            "Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.5) ...\n",
            "Setting up gnupg2 (2.2.4-1ubuntu1.4) ...\n",
            "Setting up libglib2.0-data (2.56.4-0ubuntu0.18.04.7) ...\n",
            "Setting up libcublas10 (10.2.3.254-1) ...\n",
            "Setting up libcublas-dev (10.2.3.254-1) ...\n",
            "Setting up r-cran-rvest (1.0.0-1cran1.1804.0) ...\n",
            "Setting up libperl5.26:amd64 (5.26.1-6ubuntu0.5) ...\n",
            "Setting up libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.10) ...\n",
            "Setting up r-cran-waldo (0.2.5-1cran1.1804.0) ...\n",
            "Setting up r-cran-dplyr (1.0.5-1cran1.1804.0) ...\n",
            "Setting up openssl (1.1.1-1ubuntu2.1~18.04.8) ...\n",
            "Setting up r-cran-pillar (1.5.1-1cran1.1804.0) ...\n",
            "Setting up libglib2.0-bin (2.56.4-0ubuntu0.18.04.7) ...\n",
            "Setting up libglib2.0-dev-bin (2.56.4-0ubuntu0.18.04.7) ...\n",
            "Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.5) ...\n",
            "Setting up libglib2.0-dev:amd64 (2.56.4-0ubuntu0.18.04.7) ...\n",
            "Setting up perl (5.26.1-6ubuntu0.5) ...\n",
            "Setting up git (1:2.17.1-1ubuntu0.8) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.5) ...\n",
            "Setting up binutils (2.30-21ubuntu1~18.04.5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2xx9gTbSMQK"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q2etGjaMgCG",
        "outputId": "36965d62-cf74-401d-cd85-b7f172c1d437"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed -q pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed -q spark-nlp==2.5.4\n",
        "\n",
        "! pip install --user -U nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_282\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 67kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 15.1MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.9MB/s \n",
            "\u001b[?25hCollecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434675 sha256=70676ea1e6ab52ab70def11a6c7902858d1d7280e31addf88c8b04ae3b237b42\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afYt9FmEH53G",
        "outputId": "c2f4cddd-262e-4fa4-f35d-fd8ecd5417a8"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf, SQLContext\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Column, SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "#from pyspark_llap import HiveWarehouseSession\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import *\n",
        "import pandas as pd \n",
        "\n",
        "from re import sub\n",
        "\n",
        "import sparknlp\n",
        " \n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        " \n",
        "from sparknlp.annotator import *\n",
        " \n",
        "from sparknlp.common import *\n",
        " \n",
        "from sparknlp.base import *\n",
        " \n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        " \n",
        "import nltk\n",
        " \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from pyspark.sql import functions as F"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2RzyRTnNGjx",
        "outputId": "f862bfdd-e664-4230-ad12-157b4721f627"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version; \", spark.version)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version:  2.5.4\n",
            "Apache Spark version;  2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0yrTMJaQWqz"
      },
      "source": [
        "# Mount Drive and read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD74i7zONK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181dd64c-08aa-4cdd-f31f-3f3b00b40232"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS0s8wYL25E9"
      },
      "source": [
        "Some of the sparkNLP annotators were having trouble reading spaces in path, hence used symbolic link to replace the path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSDHjKbOmpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a592cec-74af-4f83-e515-b5ab505801a4"
      },
      "source": [
        "! ln -s \"/content/drive/My Drive\" \"/content/MyDrive\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ln: failed to create symbolic link '/content/MyDrive/My Drive': File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7sjN-NzO1tx"
      },
      "source": [
        "\n",
        "song_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/content/drive/My Drive/SparkNLP/song_2k.csv\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p--pxgIPSQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c206e1e-31e1-4f04-c14d-b0a8494399db"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "song_data = song_data.select(['Key', 'Lyric', 'Genre']).withColumnRenamed('Lyric','text')\n",
        "\n",
        "from pyspark.sql.functions import monotonically_increasing_id \n",
        "\n",
        "song_data = song_data.select(\"*\").withColumn(\"Id\", monotonically_increasing_id())\n",
        "\n",
        "# train, test = trainDataset.randomSplit(weights=[0.5, 0.5], seed=123)\n",
        "song_data = song_data.limit(999)\n",
        "print(song_data.count())\n",
        "\n",
        "song_data = song_data.filter(song_data.text != '')\n",
        "print(song_data.count())\n",
        "song_data = song_data.filter((song_data.Genre == 'Rock') | (song_data.Genre == 'Hip Hop') | (song_data.Genre == 'Pop')  )\n",
        "# song_data = song_data.filter(song_data.Genre != '')\n",
        "print(song_data.count())\n",
        "# song_data = song_data.filter(song_data.text != ' ')\n",
        "# print(song_data.count())\n",
        "# song_data = song_data.filter(song_data.Genre != ' ')\n",
        "# print(song_data.count())\n",
        "song_data.na.drop(subset=[\"text\"])\n",
        "print(song_data.count())\n",
        "song_data.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n",
            "999\n",
            "915\n",
            "915\n",
            "+--------------------+--------------------+-------+---+\n",
            "|                 Key|                text|  Genre| Id|\n",
            "+--------------------+--------------------+-------+---+\n",
            "|10000 maniacs_Mor...|I could feel at t...|   Rock|  0|\n",
            "|10000 maniacs_Bec...|Take me now, baby...|   Rock|  1|\n",
            "|jamiroquai_Rock D...|And it's coming a...|    Pop|  3|\n",
            "|10000 maniacs_The...|These are. These ...|   Rock|  4|\n",
            "|10000 maniacs_Eve...|Trudging slowly o...|   Rock|  6|\n",
            "|10000 maniacs_Don...|Don't talk, I wil...|   Rock|  7|\n",
            "|black veil brides...|Have we begun to ...|   Rock|  8|\n",
            "|lynyrd skynyrd_I ...|Ain't no need to ...|   Rock|  9|\n",
            "|10000 maniacs_Acr...|Well they left th...|   Rock| 10|\n",
            "|10000 maniacs_Pla...|[ music: Dennis D...|   Rock| 12|\n",
            "|10000 maniacs_Rai...|On bended kneeI'v...|   Rock| 13|\n",
            "|twista_Back 2 School|[Tung Twista]. ba...|Hip Hop| 14|\n",
            "|10000 maniacs_Ant...|For whom do the b...|   Rock| 15|\n",
            "|10000 maniacs_All...|She walks alone o...|   Rock| 16|\n",
            "|10000 maniacs_Bac...|Jenny. Jenny you ...|   Rock| 17|\n",
            "|cyndi lauper_True...|You with the sad ...|    Pop| 18|\n",
            "|10000 maniacs_A R...|You were looking ...|   Rock| 19|\n",
            "|rick astley_She M...|She makes me more...|    Pop| 20|\n",
            "|steve earle_Nothi...|I'm the keeper of...|   Rock| 22|\n",
            "|10000 maniacs_Mad...|\"the legs of Madd...|   Rock| 24|\n",
            "+--------------------+--------------------+-------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyXMOyWZP4Xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747a02f4-9f87-424f-cd38-01c22d322b87"
      },
      "source": [
        "trainDataset= song_data.select('Id','text').withColumnRenamed('text','talk_txt').withColumnRenamed('Id','call_transcription_id')\n",
        "trainDataset.show()\n",
        "#print(type(song_data))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+--------------------+\n",
            "|call_transcription_id|            talk_txt|\n",
            "+---------------------+--------------------+\n",
            "|                    0|I could feel at t...|\n",
            "|                    1|Take me now, baby...|\n",
            "|                    3|And it's coming a...|\n",
            "|                    4|These are. These ...|\n",
            "|                    6|Trudging slowly o...|\n",
            "|                    7|Don't talk, I wil...|\n",
            "|                    8|Have we begun to ...|\n",
            "|                    9|Ain't no need to ...|\n",
            "|                   10|Well they left th...|\n",
            "|                   12|[ music: Dennis D...|\n",
            "|                   13|On bended kneeI'v...|\n",
            "|                   14|[Tung Twista]. ba...|\n",
            "|                   15|For whom do the b...|\n",
            "|                   16|She walks alone o...|\n",
            "|                   17|Jenny. Jenny you ...|\n",
            "|                   18|You with the sad ...|\n",
            "|                   19|You were looking ...|\n",
            "|                   20|She makes me more...|\n",
            "|                   22|I'm the keeper of...|\n",
            "|                   24|\"the legs of Madd...|\n",
            "+---------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1CmOapAHcJq",
        "outputId": "e5e472ff-a4a8-4f8f-be6b-4b3c105ae363"
      },
      "source": [
        "trainDataset= trainDataset.filter(trainDataset.talk_txt != \"\" )\n",
        "\n",
        "@udf(StringType())\n",
        "def collapse_text(val):\n",
        "    return '. '.join(val)\n",
        "\n",
        "\n",
        "df2 = trainDataset.groupBy(\"call_transcription_id\").agg(F.collect_list(\"talk_txt\"))\n",
        "df2.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+----------------------+\n",
            "|call_transcription_id|collect_list(talk_txt)|\n",
            "+---------------------+----------------------+\n",
            "|                    0|  [I could feel at ...|\n",
            "|                    1|  [Take me now, bab...|\n",
            "|                    3|  [And it's coming ...|\n",
            "|                    4|  [These are. These...|\n",
            "|                    6|  [Trudging slowly ...|\n",
            "|                    7|  [Don't talk, I wi...|\n",
            "|                    8|  [Have we begun to...|\n",
            "|                    9|  [Ain't no need to...|\n",
            "|                   10|  [Well they left t...|\n",
            "|                   12|  [[ music: Dennis ...|\n",
            "|                   13|  [On bended kneeI'...|\n",
            "|                   14|  [[Tung Twista]. b...|\n",
            "|                   15|  [For whom do the ...|\n",
            "|                   16|  [She walks alone ...|\n",
            "|                   17|  [Jenny. Jenny you...|\n",
            "|                   18|  [You with the sad...|\n",
            "|                   19|  [You were looking...|\n",
            "|                   20|  [She makes me mor...|\n",
            "|                   22|  [I'm the keeper o...|\n",
            "|                   24|  [\"the legs of Mad...|\n",
            "+---------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhrTbCcKHmPW",
        "outputId": "2a3e3111-3fba-4b7d-a303-92c5ab864651"
      },
      "source": [
        "df3 = df2.select(\"call_transcription_id\", collapse_text(\"collect_list(talk_txt)\").alias(\"talk_txt\"))\n",
        "df3.show()\n",
        "trainDataset=df3\n",
        "print(\"dataset loaded\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+--------------------+\n",
            "|call_transcription_id|            talk_txt|\n",
            "+---------------------+--------------------+\n",
            "|                    0|I could feel at t...|\n",
            "|                    1|Take me now, baby...|\n",
            "|                    3|And it's coming a...|\n",
            "|                    4|These are. These ...|\n",
            "|                    6|Trudging slowly o...|\n",
            "|                    7|Don't talk, I wil...|\n",
            "|                    8|Have we begun to ...|\n",
            "|                    9|Ain't no need to ...|\n",
            "|                   10|Well they left th...|\n",
            "|                   12|[ music: Dennis D...|\n",
            "|                   13|On bended kneeI'v...|\n",
            "|                   14|[Tung Twista]. ba...|\n",
            "|                   15|For whom do the b...|\n",
            "|                   16|She walks alone o...|\n",
            "|                   17|Jenny. Jenny you ...|\n",
            "|                   18|You with the sad ...|\n",
            "|                   19|You were looking ...|\n",
            "|                   20|She makes me more...|\n",
            "|                   22|I'm the keeper of...|\n",
            "|                   24|\"the legs of Madd...|\n",
            "+---------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "dataset loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eclTUBs8IM0Z",
        "outputId": "6c81a67c-6604-4ace-adbd-5727991be2ce"
      },
      "source": [
        "assembler = DocumentAssembler().setInputCol('talk_txt').setOutputCol('document')\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('token')#.setTargetPattern('/\\b(\\?You were looking)\\b/')#.setExceptionsPath('/content/MyDrive/SparkNLP/entities.txt')\n",
        "\n",
        "#spell_chk = NorvigSweetingModel().pretrained().setInputCols(['token']).setOutputCol('corrected')\n",
        "\n",
        "lemmatizer = LemmatizerModel().pretrained().setInputCols(['token']).setOutputCol('lemma')\n",
        "\n",
        "normalizer = Normalizer().setInputCols(['lemma']).setOutputCol('normalized').setLowercase(True)\n",
        "\n",
        "stop_wrd = list( stopwords.words('english'))\n",
        "\n",
        "stop_words_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False).setStopWords(stop_wrd)\n",
        "\n",
        " \n",
        "finisher = Finisher().setInputCols(['cleanTokens']).setOutputCols(['cleanTokens']).setOutputAsArray(True)\n",
        " \n",
        "pipeline_bow = Pipeline().setStages([assembler, tokenizer ,lemmatizer, normalizer,stop_words_cleaner,finisher])\n",
        " \n",
        "model_trans_tfIdf1 = pipeline_bow.fit(trainDataset)\n",
        "\n",
        "model_trans_tfIdf1.write().overwrite().save(\"/content/drive/My Drive/SparkNLP/tmp/vocab_reduction_1\")\n",
        "model_trans_tfId = PipelineModel.load(\"/content/MyDrive/SparkNLP/tmp/vocab_reduction_1\") \n",
        "model_trans_tfIdf =  model_trans_tfId.transform(trainDataset)#.persist()\n",
        " \n",
        "model_trans_tfIdf.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "+---------------------+--------------------+--------------------+\n",
            "|call_transcription_id|            talk_txt|         cleanTokens|\n",
            "+---------------------+--------------------+--------------------+\n",
            "|                    0|I could feel at t...|[could, feel, tim...|\n",
            "|                    1|Take me now, baby...|[take, baby, hold...|\n",
            "|                    3|And it's coming a...|[come, baby, yeah...|\n",
            "|                    4|These are. These ...|[day, youll, reme...|\n",
            "|                    6|Trudging slowly o...|[trudging, slowly...|\n",
            "|                    7|Don't talk, I wil...|[dont, talk, list...|\n",
            "|                    8|Have we begun to ...|[begin, drift, aw...|\n",
            "|                    9|Ain't no need to ...|[aint, need, worr...|\n",
            "|                   10|Well they left th...|[well, leave, mor...|\n",
            "|                   12|[ music: Dennis D...|[music, dennis, d...|\n",
            "|                   13|On bended kneeI'v...|[bend, kneeive, l...|\n",
            "|                   14|[Tung Twista]. ba...|[tung, twista, ba...|\n",
            "|                   15|For whom do the b...|[bell, toll, sent...|\n",
            "|                   16|She walks alone o...|[walk, alone, bri...|\n",
            "|                   17|Jenny. Jenny you ...|[jenny, jenny, do...|\n",
            "|                   18|You with the sad ...|[sad, eye, dont, ...|\n",
            "|                   19|You were looking ...|[look, away, west...|\n",
            "|                   20|She makes me more...|[make, could, eve...|\n",
            "|                   22|I'm the keeper of...|[im, keeper, hear...|\n",
            "|                   24|\"the legs of Madd...|[leg, maddox, kit...|\n",
            "+---------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIC4Hcn2J8i4",
        "outputId": "daf2452d-9d0a-429f-f360-4b83bbe72ef6"
      },
      "source": [
        "f =open(\"/content/drive/My Drive/SparkNLP/stopwords_pd.txt\", \"r\")\n",
        "sw=f.read()\n",
        "\n",
        "#df = spark.read.csv(\"/user/eb5377/stopwords_pd.txt\")\n",
        "@udf(StringType())\n",
        "def sp(x,k):\n",
        "    import re\n",
        "    x=x.lower()\n",
        "    #df = spark.read.csv(\"/user/eb5377/stopwords_pd.txt\")\n",
        "    k=k.split('\\n')\n",
        "    #stop_wrd = pd.read_csv('/home/EB5377/stopwords_pd.txt')\n",
        "    for z in k:\n",
        "        x=re.sub(z,' ',x)\n",
        "    \n",
        "    x=re.sub('/s+',' ',x)\n",
        "    x=x.strip()\n",
        "    return(x)\n",
        "\n",
        "@udf(StringType())\n",
        "def f(x):\n",
        "    return str(' '.join(x))\n",
        "\n",
        "\n",
        "@udf(IntegerType())\n",
        "def sl(x):\n",
        "    return len(x)\n",
        "\n",
        "\n",
        "model_trans_tfIdf= model_trans_tfIdf.select('talk_txt','call_transcription_id','cleanTokens',f(\"cleanTokens\").alias(\"talk_txt2\"),sl(\"talk_txt\").alias(\"str_len\"))\n",
        "model_trans_tfIdf=model_trans_tfIdf.withColumn(\"sw\",lit(sw))\n",
        "\n",
        "model_trans_tfIdf.cache()\n",
        "model_trans_tfIdf.show()\n",
        "trainDataset= model_trans_tfIdf.select('talk_txt','call_transcription_id','cleanTokens',\"str_len\",sp(\"talk_txt2\",\"sw\").alias(\"talk_txt3\"))\n",
        "\n",
        "trainDataset.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+--------------------+--------------------+-------+--------------------+\n",
            "|            talk_txt|call_transcription_id|         cleanTokens|           talk_txt2|str_len|                  sw|\n",
            "+--------------------+---------------------+--------------------+--------------------+-------+--------------------+\n",
            "|I could feel at t...|                    0|[could, feel, tim...|could feel time w...|    755|\\bapproximately\\b...|\n",
            "|Take me now, baby...|                    1|[take, baby, hold...|take baby hold cl...|   1253|\\bapproximately\\b...|\n",
            "|And it's coming a...|                    3|[come, baby, yeah...|come baby yeah co...|   1744|\\bapproximately\\b...|\n",
            "|These are. These ...|                    4|[day, youll, reme...|day youll remembe...|    884|\\bapproximately\\b...|\n",
            "|Trudging slowly o...|                    6|[trudging, slowly...|trudging slowly w...|    650|\\bapproximately\\b...|\n",
            "|Don't talk, I wil...|                    7|[dont, talk, list...|dont talk listen ...|   1351|\\bapproximately\\b...|\n",
            "|Have we begun to ...|                    8|[begin, drift, aw...|begin drift away ...|   1775|\\bapproximately\\b...|\n",
            "|Ain't no need to ...|                    9|[aint, need, worr...|aint need worry a...|    803|\\bapproximately\\b...|\n",
            "|Well they left th...|                   10|[well, leave, mor...|well leave mornin...|    645|\\bapproximately\\b...|\n",
            "|[ music: Dennis D...|                   12|[music, dennis, d...|music dennis drew...|    737|\\bapproximately\\b...|\n",
            "|On bended kneeI'v...|                   13|[bend, kneeive, l...|bend kneeive look...|    647|\\bapproximately\\b...|\n",
            "|[Tung Twista]. ba...|                   14|[tung, twista, ba...|tung twista back ...|   3090|\\bapproximately\\b...|\n",
            "|For whom do the b...|                   15|[bell, toll, sent...|bell toll sentenc...|   1011|\\bapproximately\\b...|\n",
            "|She walks alone o...|                   16|[walk, alone, bri...|walk alone brick ...|   1022|\\bapproximately\\b...|\n",
            "|Jenny. Jenny you ...|                   17|[jenny, jenny, do...|jenny jenny dont ...|   1799|\\bapproximately\\b...|\n",
            "|You with the sad ...|                   18|[sad, eye, dont, ...|sad eye dont disc...|   1298|\\bapproximately\\b...|\n",
            "|You were looking ...|                   19|[look, away, west...|look away western...|    671|\\bapproximately\\b...|\n",
            "|She makes me more...|                   20|[make, could, eve...|make could ever m...|   1289|\\bapproximately\\b...|\n",
            "|I'm the keeper of...|                   22|[im, keeper, hear...|im keeper heart k...|    608|\\bapproximately\\b...|\n",
            "|\"the legs of Madd...|                   24|[leg, maddox, kit...|leg maddox kitche...|   1411|\\bapproximately\\b...|\n",
            "+--------------------+---------------------+--------------------+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+\n",
            "|            talk_txt|call_transcription_id|         cleanTokens|str_len|           talk_txt3|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+\n",
            "|I could feel at t...|                    0|[could, feel, tim...|    755|could feel time w...|\n",
            "|Take me now, baby...|                    1|[take, baby, hold...|   1253|take baby hold cl...|\n",
            "|And it's coming a...|                    3|[come, baby, yeah...|   1744|come baby yeah co...|\n",
            "|These are. These ...|                    4|[day, youll, reme...|    884|day youll remembe...|\n",
            "|Trudging slowly o...|                    6|[trudging, slowly...|    650|trudging slowly w...|\n",
            "|Don't talk, I wil...|                    7|[dont, talk, list...|   1351|dont talk listen ...|\n",
            "|Have we begun to ...|                    8|[begin, drift, aw...|   1775|begin drift away ...|\n",
            "|Ain't no need to ...|                    9|[aint, need, worr...|    803|aint need worry a...|\n",
            "|Well they left th...|                   10|[well, leave, mor...|    645|well leave mornin...|\n",
            "|[ music: Dennis D...|                   12|[music, dennis, d...|    737|music dennis drew...|\n",
            "|On bended kneeI'v...|                   13|[bend, kneeive, l...|    647|bend kneeive look...|\n",
            "|[Tung Twista]. ba...|                   14|[tung, twista, ba...|   3090|tung twista back ...|\n",
            "|For whom do the b...|                   15|[bell, toll, sent...|   1011|bell toll sentenc...|\n",
            "|She walks alone o...|                   16|[walk, alone, bri...|   1022|walk alone brick ...|\n",
            "|Jenny. Jenny you ...|                   17|[jenny, jenny, do...|   1799|jenny jenny dont ...|\n",
            "|You with the sad ...|                   18|[sad, eye, dont, ...|   1298|sad eye dont disc...|\n",
            "|You were looking ...|                   19|[look, away, west...|    671|look away western...|\n",
            "|She makes me more...|                   20|[make, could, eve...|   1289|could ever   feel...|\n",
            "|I'm the keeper of...|                   22|[im, keeper, hear...|    608|im keeper heart k...|\n",
            "|\"the legs of Madd...|                   24|[leg, maddox, kit...|   1411|leg maddox kitche...|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmGJ_ZJ6Mo5y"
      },
      "source": [
        "trainDataset= trainDataset.filter(trainDataset.talk_txt3 != \"\" )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdYbFOG0M0L7",
        "outputId": "2d8c7f8b-3ae8-44a0-b1eb-785f4c93c41c"
      },
      "source": [
        "#round 2\n",
        " \n",
        "assembler = DocumentAssembler().setInputCol('talk_txt3').setOutputCol('document')\n",
        " \n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        " \n",
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('token')\n",
        "\n",
        "#spell_chk = NorvigSweetingModel.load(\"/datahube/sandbox/datascience/entds/staging/nlp_models/models/spellcheck_norvig_en_2.0.2_2.4_1556605026653\").setInputCols([\"token\"]).setOutputCol(\"corrected\")\n",
        "\n",
        " \n",
        "#spell_chk = NorvigSweetingModel().pretrained().setInputCols(['token']).setOutputCol('corrected')\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"token\"]).setOutputCol(\"lemma\") \n",
        "#lemmatizer = LemmatizerModel().pretrained().setInputCols(['corrected']).setOutputCol('lemma')\n",
        " \n",
        "normalizer = Normalizer().setInputCols(['lemma']).setOutputCol('normalized').setLowercase(True)\n",
        "\n",
        "#stop_wrd = spark.sparkContext.textFile(\"/user/eb5377/stopwords.txt\").collect() \n",
        "stop_wrd = list( stopwords.words('english'))\n",
        " \n",
        "stop_words_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False).setStopWords(stop_wrd)\n",
        "\n",
        "\n",
        "trigram = NGramGenerator().setInputCols([\"cleanTokens\"]).setOutputCol(\"trigram\").setN(1).setEnableCumulative(False)  \n",
        "#stop_words_cleaner5 = StopWordsCleaner().setInputCols([\"trigram1\"]).setOutputCol(\"trigram\").setCaseSensitive(False).setStopWords(stop_wrd)\n",
        "\n",
        "\n",
        "finisher = Finisher().setInputCols(['trigram']).setOutputCols(['cleanTokens']).setOutputAsArray(True)\n",
        "\n",
        "pipeline_bow = Pipeline().setStages([assembler, tokenizer ,lemmatizer, normalizer,stop_words_cleaner,trigram,finisher])\n",
        "\n",
        "model_trans_tfIdf2 = pipeline_bow.fit(trainDataset)\n",
        "\n",
        "model_trans_tfIdf2.write().overwrite().save(\"/content/drive/My Drive/SparkNLP/tmp/vocab_reduction_2\")\n",
        "model_trans_tfId = PipelineModel.load(\"/content/MyDrive/SparkNLP/tmp/vocab_reduction_2\") \n",
        "\n",
        "model_trans_tfIdf =  model_trans_tfId.transform(trainDataset)#.persist()\n",
        "\n",
        "model_trans_tfIdf.show()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+\n",
            "|            talk_txt|call_transcription_id|         cleanTokens|str_len|           talk_txt3|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+\n",
            "|I could feel at t...|                    0|[could, feel, tim...|    755|could feel time w...|\n",
            "|Take me now, baby...|                    1|[take, baby, hold...|   1253|take baby hold cl...|\n",
            "|And it's coming a...|                    3|[come, baby, yeah...|   1744|come baby yeah co...|\n",
            "|These are. These ...|                    4|[day, youll, reme...|    884|day youll remembe...|\n",
            "|Trudging slowly o...|                    6|[trudge, slowly, ...|    650|trudging slowly w...|\n",
            "|Don't talk, I wil...|                    7|[dont, talk, list...|   1351|dont talk listen ...|\n",
            "|Have we begun to ...|                    8|[begin, drift, aw...|   1775|begin drift away ...|\n",
            "|Ain't no need to ...|                    9|[aint, need, worr...|    803|aint need worry a...|\n",
            "|Well they left th...|                   10|[well, leave, mor...|    645|well leave mornin...|\n",
            "|[ music: Dennis D...|                   12|[music, dennis, d...|    737|music dennis drew...|\n",
            "|On bended kneeI'v...|                   13|[bend, kneeive, l...|    647|bend kneeive look...|\n",
            "|[Tung Twista]. ba...|                   14|[tung, twista, ba...|   3090|tung twista back ...|\n",
            "|For whom do the b...|                   15|[bell, toll, sent...|   1011|bell toll sentenc...|\n",
            "|She walks alone o...|                   16|[walk, alone, bri...|   1022|walk alone brick ...|\n",
            "|Jenny. Jenny you ...|                   17|[jenny, jenny, do...|   1799|jenny jenny dont ...|\n",
            "|You with the sad ...|                   18|[sad, eye, dont, ...|   1298|sad eye dont disc...|\n",
            "|You were looking ...|                   19|[look, away, west...|    671|look away western...|\n",
            "|She makes me more...|                   20|[could, ever, fee...|   1289|could ever   feel...|\n",
            "|I'm the keeper of...|                   22|[im, keeper, hear...|    608|im keeper heart k...|\n",
            "|\"the legs of Madd...|                   24|[leg, maddox, kit...|   1411|leg maddox kitche...|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGA4enOUTzek"
      },
      "source": [
        "num_topics = 8\n",
        "sparsity_df=50\n",
        "it=80"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9BqgHyuT6ii",
        "outputId": "06bd771f-4f90-443b-cd68-3877d98d856a"
      },
      "source": [
        "#Vectorization\n",
        " \n",
        "from pyspark.ml.feature import CountVectorizer, IDF, CountVectorizerModel\n",
        "\n",
        "count_vectorizer = CountVectorizer(inputCol='cleanTokens', outputCol='tf', minDF=sparsity_df)\n",
        "\n",
        "#idf = IDF(inputCol='tf', outputCol='features', minDocFreq=10)\n",
        "\n",
        "#bow_pipeline = Pipeline().setStages([count_vectorizer])\n",
        "\n",
        "bow_pipeline1 = count_vectorizer.fit(model_trans_tfIdf)\n",
        "bow_pipeline1.write().overwrite().save('/content/drive/MyDrive/SparkNLP/tmp/count_vocab')\n",
        "\n",
        "bow_pipeline = CountVectorizerModel.load('/content/drive/MyDrive/SparkNLP/tmp/count_vocab')\n",
        "bows = bow_pipeline.transform(model_trans_tfIdf)\n",
        "\n",
        "bows = bows.withColumnRenamed(\"tf\",\"features\") \n",
        "bows.show(10) "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+--------------------+-------+--------------------+--------------------+\n",
            "|            talk_txt|call_transcription_id|         cleanTokens|str_len|           talk_txt3|            features|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+--------------------+\n",
            "|I could feel at t...|                    0|[could, feel, tim...|    755|could feel time w...|(214,[0,2,3,5,7,1...|\n",
            "|Take me now, baby...|                    1|[take, baby, hold...|   1253|take baby hold cl...|(214,[0,6,8,10,12...|\n",
            "|And it's coming a...|                    3|[come, baby, yeah...|   1744|come baby yeah co...|(214,[0,1,2,3,4,5...|\n",
            "|These are. These ...|                    4|[day, youll, reme...|    884|day youll remembe...|(214,[2,9,11,16,2...|\n",
            "|Trudging slowly o...|                    6|[trudge, slowly, ...|    650|trudging slowly w...|(214,[5,12,24,73,...|\n",
            "|Don't talk, I wil...|                    7|[dont, talk, list...|   1351|dont talk listen ...|(214,[0,2,3,4,9,1...|\n",
            "|Have we begun to ...|                    8|[begin, drift, aw...|   1775|begin drift away ...|(214,[1,3,5,7,8,1...|\n",
            "|Ain't no need to ...|                    9|[aint, need, worr...|    803|aint need worry a...|(214,[0,1,2,3,6,7...|\n",
            "|Well they left th...|                   10|[well, leave, mor...|    645|well leave mornin...|(214,[3,6,17,19,2...|\n",
            "|[ music: Dennis D...|                   12|[music, dennis, d...|    737|music dennis drew...|(214,[9,11,20,43,...|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyxyYMsDNZ4n"
      },
      "source": [
        "#LDA (Topic Modeling)\n",
        " \n",
        "from pyspark.ml.clustering import LDA, LDAModel, LocalLDAModel\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=it)\n",
        "\n",
        "model = lda.fit(bows)\n",
        "\n",
        "model.write().overwrite().save(\"/content/drive/My Drive/SparkNLP/TM_Targeted.model\")\n",
        "model1 = LocalLDAModel.load(\"/content/drive/My Drive/SparkNLP/TM_Targeted.model\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z4Wv-UVSRvo"
      },
      "source": [
        "#extract vocabulary from CountVectorizer\n",
        "\n",
        "# ******** SAVE AND LOAD THE MODEL FOR REUSE **************\n",
        " \n",
        "vocab = count_vectorizer.fit(model_trans_tfIdf).vocabulary\n",
        " \n",
        "topics = model1.describeTopics()   \n",
        " \n",
        "topics_rdd = topics.rdd\n",
        " \n",
        "topics_words = topics_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
        "topic_cluster = ''\n",
        "\n",
        "for idx, topic in enumerate(topics_words):\n",
        "    #print(\"topic: {}\".format(idx))\n",
        "    s1 = \"topic: {}\".format(idx) \n",
        "    #print(\"*\"*25)\n",
        "    s2 = '\\n' + \"*\"*25\n",
        "    s3 = ''\n",
        "    for word in topic:\n",
        "        #print(word)\n",
        "        s3 =  s3 + '\\n' + word\n",
        "    s4 =  '\\n' + \"*\"*25  + '\\n'\n",
        "    #print(\"*\"*25)\n",
        "    topic_cluster = topic_cluster + s1 +s2 +s3 +s4\n",
        "\n",
        "\n",
        "print(topic_cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npJQXHAJS2X7",
        "outputId": "05ada295-2bdc-4cc6-ff72-39e54128e992"
      },
      "source": [
        "lda_model = model1.transform(bows)\n",
        "lda_model.show()\n",
        "#lda_model.select('features','topicDistribution').show( truncate = False)\n",
        "\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "from pyspark.sql.types import *\n",
        "import numpy as np\n",
        "\n",
        "@udf(IntegerType())\n",
        "def dominant_topics(lst_per):\n",
        "    lst_per = np.asarray(lst_per.values) #.tolist()\n",
        "    if float(np.max(lst_per)) == 0.0:\n",
        "            k=int(9999)\n",
        "    else:\n",
        "            k =  int(np.argmax(lst_per))\n",
        "    return int(k)\n",
        "\n",
        "\n",
        "@udf(FloatType())\n",
        "def percent_contrib(lst_per):\n",
        "    lst_per = np.asarray(lst_per.values) #.tolist()\n",
        "    if float(np.max(lst_per)) == 0.0:\n",
        "            k=-9.99\n",
        "    else:\n",
        "            k =  float(np.max(lst_per))\n",
        "    return float(k)\n",
        "\n",
        "\n",
        "dom_topic_model = lda_model.select('call_transcription_id','talk_txt','str_len','talk_txt3','topicDistribution',dominant_topics(\"topicDistribution\").alias(\"dom_topic\"),percent_contrib(\"topicDistribution\").alias(\"dom_per\"))\n",
        "dom_topic_model= dom_topic_model.filter(dom_topic_model.dom_topic != 9999 )\n",
        "dom_topic_model.cache()\n",
        "dom_topic_model.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+--------------------+-------+--------------------+--------------------+--------------------+\n",
            "|            talk_txt|call_transcription_id|         cleanTokens|str_len|           talk_txt3|            features|   topicDistribution|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+--------------------+--------------------+\n",
            "|I could feel at t...|                    0|[could, feel, tim...|    755|could feel time w...|(214,[0,2,3,5,7,1...|[0.00245431377749...|\n",
            "|Take me now, baby...|                    1|[take, baby, hold...|   1253|take baby hold cl...|(214,[0,6,8,10,12...|[0.00194776512034...|\n",
            "|And it's coming a...|                    3|[come, baby, yeah...|   1744|come baby yeah co...|(214,[0,1,2,3,4,5...|[0.25192971282655...|\n",
            "|These are. These ...|                    4|[day, youll, reme...|    884|day youll remembe...|(214,[2,9,11,16,2...|[0.00250290313505...|\n",
            "|Trudging slowly o...|                    6|[trudge, slowly, ...|    650|trudging slowly w...|(214,[5,12,24,73,...|[0.00682558299493...|\n",
            "|Don't talk, I wil...|                    7|[dont, talk, list...|   1351|dont talk listen ...|(214,[0,2,3,4,9,1...|[0.00142668397423...|\n",
            "|Have we begun to ...|                    8|[begin, drift, aw...|   1775|begin drift away ...|(214,[1,3,5,7,8,1...|[0.57839974919945...|\n",
            "|Ain't no need to ...|                    9|[aint, need, worr...|    803|aint need worry a...|(214,[0,1,2,3,6,7...|[0.66909270448310...|\n",
            "|Well they left th...|                   10|[well, leave, mor...|    645|well leave mornin...|(214,[3,6,17,19,2...|[0.00371930994582...|\n",
            "|[ music: Dennis D...|                   12|[music, dennis, d...|    737|music dennis drew...|(214,[9,11,20,43,...|[0.00877171230487...|\n",
            "|On bended kneeI'v...|                   13|[bend, kneeive, l...|    647|bend kneeive look...|(214,[5,6,12,23,3...|[0.00331812738139...|\n",
            "|[Tung Twista]. ba...|                   14|[tung, twista, ba...|   3090|tung twista back ...|(214,[0,1,2,3,5,6...|[0.29882546585955...|\n",
            "|For whom do the b...|                   15|[bell, toll, sent...|   1011|bell toll sentenc...|(214,[4,5,9,11,15...|[0.38355525482718...|\n",
            "|She walks alone o...|                   16|[walk, alone, bri...|   1022|walk alone brick ...|(214,[2,3,5,6,9,1...|[0.00240776595691...|\n",
            "|Jenny. Jenny you ...|                   17|[jenny, jenny, do...|   1799|jenny jenny dont ...|(214,[0,1,2,4,7,9...|[0.00157297355075...|\n",
            "|You with the sad ...|                   18|[sad, eye, dont, ...|   1298|sad eye dont disc...|(214,[2,4,5,6,8,1...|[0.03330366733313...|\n",
            "|You were looking ...|                   19|[look, away, west...|    671|look away western...|(214,[0,2,7,11,12...|[0.00314794522229...|\n",
            "|She makes me more...|                   20|[could, ever, fee...|   1289|could ever   feel...|(214,[0,5,6,9,11,...|[0.00151478697233...|\n",
            "|I'm the keeper of...|                   22|[im, keeper, hear...|    608|im keeper heart k...|(214,[0,4,7,10,18...|[0.52598994910292...|\n",
            "|\"the legs of Madd...|                   24|[leg, maddox, kit...|   1411|leg maddox kitche...|(214,[3,11,20,21,...|[0.00350799081291...|\n",
            "+--------------------+---------------------+--------------------+-------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------------------+--------------------+-------+--------------------+--------------------+---------+----------+\n",
            "|call_transcription_id|            talk_txt|str_len|           talk_txt3|   topicDistribution|dom_topic|   dom_per|\n",
            "+---------------------+--------------------+-------+--------------------+--------------------+---------+----------+\n",
            "|                    0|I could feel at t...|    755|could feel time w...|[0.00245431377749...|        5|0.64520043|\n",
            "|                    1|Take me now, baby...|   1253|take baby hold cl...|[0.00194776512034...|        1|0.98726135|\n",
            "|                    3|And it's coming a...|   1744|come baby yeah co...|[0.25192971282655...|        1| 0.4496019|\n",
            "|                    4|These are. These ...|    884|day youll remembe...|[0.00250290313505...|        4|0.95779765|\n",
            "|                    6|Trudging slowly o...|    650|trudging slowly w...|[0.00682558299493...|        1|  0.660717|\n",
            "|                    7|Don't talk, I wil...|   1351|dont talk listen ...|[0.00142668397423...|        1|0.58556086|\n",
            "|                    8|Have we begun to ...|   1775|begin drift away ...|[0.57839974919945...|        0| 0.5783998|\n",
            "|                    9|Ain't no need to ...|    803|aint need worry a...|[0.66909270448310...|        0| 0.6690927|\n",
            "|                   10|Well they left th...|    645|well leave mornin...|[0.00371930994582...|        6|0.40230504|\n",
            "|                   12|[ music: Dennis D...|    737|music dennis drew...|[0.00877171230487...|        3| 0.5734105|\n",
            "|                   13|On bended kneeI'v...|    647|bend kneeive look...|[0.00331812738139...|        5|0.36639273|\n",
            "|                   14|[Tung Twista]. ba...|   3090|tung twista back ...|[0.29882546585955...|        0|0.29882547|\n",
            "|                   15|For whom do the b...|   1011|bell toll sentenc...|[0.38355525482718...|        0|0.38355526|\n",
            "|                   16|She walks alone o...|   1022|walk alone brick ...|[0.00240776595691...|        6|0.27475977|\n",
            "|                   17|Jenny. Jenny you ...|   1799|jenny jenny dont ...|[0.00157297355075...|        5|0.40175816|\n",
            "|                   18|You with the sad ...|   1298|sad eye dont disc...|[0.03330366733313...|        4| 0.5670944|\n",
            "|                   19|You were looking ...|    671|look away western...|[0.00314794522229...|        3|0.32942957|\n",
            "|                   20|She makes me more...|   1289|could ever   feel...|[0.00151478697233...|        5|0.66541827|\n",
            "|                   22|I'm the keeper of...|    608|im keeper heart k...|[0.52598994910292...|        0|0.52598995|\n",
            "|                   24|\"the legs of Madd...|   1411|leg maddox kitche...|[0.00350799081291...|        4| 0.3607559|\n",
            "+---------------------+--------------------+-------+--------------------+--------------------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "Yvhc1_NWTmLH",
        "outputId": "e50ce12f-deb8-486d-d62e-3143b7155622"
      },
      "source": [
        "header='topic|Sample #|Call Transcription ID|Original Text|Model Text|Percent Relevant\\n'\n",
        "\n",
        "phrases_ex=header\n",
        "\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    s1 = str(\"topic: {}\".format(idx))\n",
        "    #s2 = '\\n' + \"*\"*25\n",
        "    s3 = '|'\n",
        "    s4=''\n",
        "    tmp= dom_topic_model.filter(dom_topic_model.dom_topic ==idx) #.filter(dom_topic_model.str_len<=400)\n",
        "    tmp.cache()\n",
        "    tmp=tmp.sort(tmp.dom_per.desc())\n",
        "    txt=tmp.select('talk_txt').collect()    \n",
        "    txt = [ ele.__getattr__('talk_txt') for ele in txt]\n",
        "    cid=tmp.select('call_transcription_id').collect()    \n",
        "    cid = [ ele.__getattr__('call_transcription_id') for ele in cid]\n",
        "    mtxt=tmp.select('talk_txt3').collect()    \n",
        "    mtxt = [ ele.__getattr__('talk_txt3') for ele in mtxt]\n",
        "    per=tmp.select('dom_per').collect()\n",
        "    per = [ ele.__getattr__('dom_per') for ele in per]\n",
        "    if len(txt)<40:\n",
        "        cnt = len(txt)\n",
        "    else:\n",
        "        cnt = 40\n",
        "    for x in range(cnt):\n",
        "        z=s1+s3+str(x) + s3+ str(cid[x]) +s3+  str(txt[x])+s3+ str(mtxt[x])+s3 +  str(\"{:.3f}\".format(100.0*per[x])) +'%' +'\\n'\n",
        "        s4 =  s4 + z\n",
        "    #print(\"*\"*25)\n",
        "    phrases_ex = phrases_ex +s4\n",
        "\n",
        "\n",
        "print(phrases_ex)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-6f1399f6697d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmtxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk_txt3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk_txt3'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmtxt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dom_per'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dom_per'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpt7Gi9F-NrX"
      },
      "source": [
        "CREATE TEST DOCUMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pudCuOeo-JJv"
      },
      "source": [
        "model_trans_tfIdf_TEST = model_trans_tfIdf.SELECT()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCy2PK74-ITO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}