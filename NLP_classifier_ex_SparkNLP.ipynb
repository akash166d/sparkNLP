{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_classifier_ex_SparkNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOx825wMmxzbGTj+uHLKUmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash166d/sparkNLP/blob/master/NLP_classifier_ex_SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBmc0480QLzB",
        "colab_type": "text"
      },
      "source": [
        "# Install Package/Lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwc2OqTH16VJ",
        "colab_type": "text"
      },
      "source": [
        "This is required to ensure the ubuntu related dependencies are up to date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQwcI3O3KCty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d24dfc9-19ff-44d4-cb2c-c91306892645"
      },
      "source": [
        "! sudo apt-get update --fix-missing\n",
        "! sudo apt-get upgrade --fix-missing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [882 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,558 B]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,038 kB]\n",
            "Get:18 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,856 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,413 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,334 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [116 kB]\n",
            "Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [895 kB]\n",
            "Fetched 8,099 kB in 3s (2,370 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following packages have been kept back:\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "  r-cran-tidyr\n",
            "The following packages will be upgraded:\n",
            "  base-files binutils binutils-common binutils-x86-64-linux-gnu bsdutils\n",
            "  cuda-compat-10-1 e2fsprogs fdisk kmod libbinutils libblkid1 libc-bin\n",
            "  libcom-err2 libext2fs2 libfdisk1 libgcrypt20 libgnutls30 libkmod2\n",
            "  libldap-2.4-2 libldap-common libmount1 libnss3 libpam-systemd libpulse0\n",
            "  libsasl2-2 libsasl2-modules-db libseccomp2 libsmartcols1 libsqlite3-0 libss2\n",
            "  libssh-gcrypt-4 libsystemd0 libudev1 linux-libc-dev module-init-tools mount\n",
            "  openssl python3-software-properties r-cran-dplyr r-cran-dt r-cran-fs\n",
            "  r-cran-ps software-properties-common systemd systemd-sysv udev util-linux\n",
            "47 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 23.1 MB of archives.\n",
            "After this operation, 315 kB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-compat-10-1 418.152.00-1 [5,246 kB]\n",
            "Get:2 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-dplyr amd64 1.0.1-1cran1.1804.0 [1,040 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 base-files amd64 10.1ubuntu2.9 [59.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 bsdutils amd64 1:2.31.1-0.4ubuntu3.6 [60.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libext2fs2 amd64 1.44.1-1ubuntu1.3 [157 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 e2fsprogs amd64 1.44.1-1ubuntu1.3 [391 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libblkid1 amd64 2.31.1-0.4ubuntu3.6 [124 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libfdisk1 amd64 2.31.1-0.4ubuntu3.6 [164 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmount1 amd64 2.31.1-0.4ubuntu3.6 [136 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsmartcols1 amd64 2.31.1-0.4ubuntu3.6 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 fdisk amd64 2.31.1-0.4ubuntu3.6 [108 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 util-linux amd64 2.31.1-0.4ubuntu3.6 [903 kB]\n",
            "Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-dt all 0.15-1cran1.1804.0 [1,199 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-bin amd64 2.27-3ubuntu1.2 [637 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 systemd-sysv amd64 237-3ubuntu10.42 [15.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam-systemd amd64 237-3ubuntu10.42 [107 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsystemd0 amd64 237-3ubuntu10.42 [208 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 systemd amd64 237-3ubuntu10.42 [2,914 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 udev amd64 237-3ubuntu10.42 [1,102 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev1 amd64 237-3ubuntu10.42 [57.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 kmod amd64 24-1ubuntu3.5 [88.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkmod2 amd64 24-1ubuntu3.5 [40.2 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mount amd64 2.31.1-0.4ubuntu3.6 [107 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcom-err2 amd64 1.44.1-1ubuntu1.3 [8,848 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgcrypt20 amd64 1.8.1-4ubuntu1.2 [417 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libss2 amd64 1.44.1-1ubuntu1.3 [11.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgnutls30 amd64 3.5.18-1ubuntu1.4 [645 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libseccomp2 amd64 2.4.3-1ubuntu3.18.04.3 [42.0 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsqlite3-0 amd64 3.22.0-1ubuntu0.4 [499 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssl amd64 1.1.1-1ubuntu2.1~18.04.6 [614 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.4 [1,839 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.4 [196 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.4 [3,392 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.4 [488 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.1 [14.8 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.1 [49.2 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.6 [17.0 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.6 [155 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnss3 amd64 2:3.35-2ubuntu2.11 [1,221 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse0 amd64 1:11.1-1ubuntu7.10 [266 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libssh-gcrypt-4 amd64 0.8.0~20170825.94fa1e38-1ubuntu0.7 [172 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-112.113 [982 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 module-init-tools all 24-1ubuntu3.5 [2,516 B]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 software-properties-common all 0.96.24.32.14 [10.1 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-software-properties all 0.96.24.32.14 [23.9 kB]\n",
            "Get:46 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-fs amd64 1.5.0-1cran1.1804.0 [254 kB]\n",
            "Get:47 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-ps amd64 1.3.4-1cran1.1804.0 [206 kB]\n",
            "Fetched 23.1 MB in 4s (5,515 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 47.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_10.1ubuntu2.9_amd64.deb ...\n",
            "Unpacking base-files (10.1ubuntu2.9) over (10.1ubuntu2.7) ...\n",
            "Setting up base-files (10.1ubuntu2.9) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../bsdutils_1%3a2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking bsdutils (1:2.31.1-0.4ubuntu3.6) over (1:2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up bsdutils (1:2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libext2fs2_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libext2fs2:amd64 (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up libext2fs2:amd64 (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../e2fsprogs_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking e2fsprogs (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up e2fsprogs (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libblkid1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libblkid1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libblkid1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libfdisk1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libfdisk1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libfdisk1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libmount1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libmount1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libmount1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libsmartcols1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libsmartcols1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libsmartcols1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../fdisk_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking fdisk (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up fdisk (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../util-linux_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking util-linux (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up util-linux (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-bin_2.27-3ubuntu1.2_amd64.deb ...\n",
            "Unpacking libc-bin (2.27-3ubuntu1.2) over (2.27-3ubuntu1) ...\n",
            "Setting up libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../systemd-sysv_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking systemd-sysv (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../libpam-systemd_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking libpam-systemd:amd64 (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../libsystemd0_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking libsystemd0:amd64 (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Setting up libsystemd0:amd64 (237-3ubuntu10.42) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../systemd_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking systemd (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../udev_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking udev (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../libudev1_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Setting up libudev1:amd64 (237-3ubuntu10.42) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../kmod_24-1ubuntu3.5_amd64.deb ...\n",
            "Unpacking kmod (24-1ubuntu3.5) over (24-1ubuntu3.4) ...\n",
            "Preparing to unpack .../libkmod2_24-1ubuntu3.5_amd64.deb ...\n",
            "Unpacking libkmod2:amd64 (24-1ubuntu3.5) over (24-1ubuntu3.4) ...\n",
            "Preparing to unpack .../mount_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking mount (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Preparing to unpack .../libcom-err2_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libcom-err2:amd64 (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up libcom-err2:amd64 (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libgcrypt20_1.8.1-4ubuntu1.2_amd64.deb ...\n",
            "Unpacking libgcrypt20:amd64 (1.8.1-4ubuntu1.2) over (1.8.1-4ubuntu1.1) ...\n",
            "Setting up libgcrypt20:amd64 (1.8.1-4ubuntu1.2) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libss2_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libss2:amd64 (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up libss2:amd64 (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.5.18-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.5.18-1ubuntu1.4) over (3.5.18-1ubuntu1.1) ...\n",
            "Setting up libgnutls30:amd64 (3.5.18-1ubuntu1.4) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libseccomp2_2.4.3-1ubuntu3.18.04.3_amd64.deb ...\n",
            "Unpacking libseccomp2:amd64 (2.4.3-1ubuntu3.18.04.3) over (2.4.1-0ubuntu0.18.04.2) ...\n",
            "Setting up libseccomp2:amd64 (2.4.3-1ubuntu3.18.04.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libsqlite3-0_3.22.0-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking libsqlite3-0:amd64 (3.22.0-1ubuntu0.4) over (3.22.0-1ubuntu0.1) ...\n",
            "Preparing to unpack .../01-openssl_1.1.1-1ubuntu2.1~18.04.6_amd64.deb ...\n",
            "Unpacking openssl (1.1.1-1ubuntu2.1~18.04.6) over (1.1.1-1ubuntu2.1~18.04.5) ...\n",
            "Preparing to unpack .../02-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../03-binutils-common_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../04-binutils_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking binutils (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../05-libbinutils_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../06-cuda-compat-10-1_418.152.00-1_amd64.deb ...\n",
            "Unpacking cuda-compat-10-1 (418.152.00-1) over (418.87.01-1) ...\n",
            "Preparing to unpack .../07-libsasl2-modules-db_2.1.27~101-g0780600+dfsg-3ubuntu2.1_amd64.deb ...\n",
            "Unpacking libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) over (2.1.27~101-g0780600+dfsg-3ubuntu2) ...\n",
            "Preparing to unpack .../08-libsasl2-2_2.1.27~101-g0780600+dfsg-3ubuntu2.1_amd64.deb ...\n",
            "Unpacking libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) over (2.1.27~101-g0780600+dfsg-3ubuntu2) ...\n",
            "Preparing to unpack .../09-libldap-common_2.4.45+dfsg-1ubuntu1.6_all.deb ...\n",
            "Unpacking libldap-common (2.4.45+dfsg-1ubuntu1.6) over (2.4.45+dfsg-1ubuntu1.4) ...\n",
            "Preparing to unpack .../10-libldap-2.4-2_2.4.45+dfsg-1ubuntu1.6_amd64.deb ...\n",
            "Unpacking libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.6) over (2.4.45+dfsg-1ubuntu1.4) ...\n",
            "Preparing to unpack .../11-libnss3_2%3a3.35-2ubuntu2.11_amd64.deb ...\n",
            "Unpacking libnss3:amd64 (2:3.35-2ubuntu2.11) over (2:3.35-2ubuntu2.9) ...\n",
            "Preparing to unpack .../12-libpulse0_1%3a11.1-1ubuntu7.10_amd64.deb ...\n",
            "Unpacking libpulse0:amd64 (1:11.1-1ubuntu7.10) over (1:11.1-1ubuntu7.9) ...\n",
            "Preparing to unpack .../13-libssh-gcrypt-4_0.8.0~20170825.94fa1e38-1ubuntu0.7_amd64.deb ...\n",
            "Unpacking libssh-gcrypt-4:amd64 (0.8.0~20170825.94fa1e38-1ubuntu0.7) over (0.8.0~20170825.94fa1e38-1ubuntu0.6) ...\n",
            "Preparing to unpack .../14-linux-libc-dev_4.15.0-112.113_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-112.113) over (4.15.0-72.81) ...\n",
            "Preparing to unpack .../15-module-init-tools_24-1ubuntu3.5_all.deb ...\n",
            "Unpacking module-init-tools (24-1ubuntu3.5) over (24-1ubuntu3.4) ...\n",
            "Preparing to unpack .../16-software-properties-common_0.96.24.32.14_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.32.14) over (0.96.24.32.13) ...\n",
            "Preparing to unpack .../17-python3-software-properties_0.96.24.32.14_all.deb ...\n",
            "Unpacking python3-software-properties (0.96.24.32.14) over (0.96.24.32.13) ...\n",
            "Preparing to unpack .../18-r-cran-dplyr_1.0.1-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-dplyr (1.0.1-1cran1.1804.0) over (1.0.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../19-r-cran-dt_0.15-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-dt (0.15-1cran1.1804.0) over (0.14-1cran1.1804.0) ...\n",
            "Preparing to unpack .../20-r-cran-fs_1.5.0-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-fs (1.5.0-1cran1.1804.0) over (1.4.2-1cran1.1804.0) ...\n",
            "Preparing to unpack .../21-r-cran-ps_1.3.4-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-ps (1.3.4-1cran1.1804.0) over (1.3.3-1cran1.1804.0) ...\n",
            "Setting up r-cran-ps (1.3.4-1cran1.1804.0) ...\n",
            "Setting up libldap-common (2.4.45+dfsg-1ubuntu1.6) ...\n",
            "Setting up cuda-compat-10-1 (418.152.00-1) ...\n",
            "Setting up libssh-gcrypt-4:amd64 (0.8.0~20170825.94fa1e38-1ubuntu0.7) ...\n",
            "Setting up libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-112.113) ...\n",
            "Setting up mount (2.31.1-0.4ubuntu3.6) ...\n",
            "Setting up libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Setting up libkmod2:amd64 (24-1ubuntu3.5) ...\n",
            "Setting up libpulse0:amd64 (1:11.1-1ubuntu7.10) ...\n",
            "Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.4) ...\n",
            "Setting up r-cran-dt (0.15-1cran1.1804.0) ...\n",
            "Setting up udev (237-3ubuntu10.42) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Setting up libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.6) ...\n",
            "Setting up r-cran-fs (1.5.0-1cran1.1804.0) ...\n",
            "Setting up systemd (237-3ubuntu10.42) ...\n",
            "Setting up r-cran-dplyr (1.0.1-1cran1.1804.0) ...\n",
            "Setting up openssl (1.1.1-1ubuntu2.1~18.04.6) ...\n",
            "Setting up libsqlite3-0:amd64 (3.22.0-1ubuntu0.4) ...\n",
            "Setting up python3-software-properties (0.96.24.32.14) ...\n",
            "Setting up software-properties-common (0.96.24.32.14) ...\n",
            "Setting up kmod (24-1ubuntu3.5) ...\n",
            "Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.4) ...\n",
            "Setting up systemd-sysv (237-3ubuntu10.42) ...\n",
            "Setting up libnss3:amd64 (2:3.35-2ubuntu2.11) ...\n",
            "Setting up module-init-tools (24-1ubuntu3.5) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.4) ...\n",
            "Setting up libpam-systemd:amd64 (237-3ubuntu10.42) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up binutils (2.30-21ubuntu1~18.04.4) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q2etGjaMgCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "7bbfb9ce-e3d2-4d44-ff9d-89691a14d15c"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed -q pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed -q spark-nlp==2.5.4\n",
        "\n",
        "! pip install --user -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_265\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 55kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 37.1MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.4MB/s \n",
            "\u001b[?25hCollecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434673 sha256=f05d72723947b9da775716adbbfb7d1e24d90b0831abdbf710ab2cc30c9ac703\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2RzyRTnNGjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "04fd0515-2957-4f31-f5a2-d2e742217075"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version; \", spark.version)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version:  2.5.4\n",
            "Apache Spark version;  2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0yrTMJaQWqz",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive and read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD74i7zONK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "8792bedc-3815-47c0-f4b2-b2a2f0e1ef5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS0s8wYL25E9",
        "colab_type": "text"
      },
      "source": [
        "Some of the sparkNLP annotators were having trouble reading spaces in path, hence used symbolic link to replace the path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSDHjKbOmpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ln -s \"/content/drive/My Drive\" \"/content/MyDrive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7sjN-NzO1tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "song_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/content/drive/My Drive/SparkNLP/song_2k.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p--pxgIPSQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "d309e48a-32cf-42b1-a50c-ec577c33bc01"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "song_data = song_data.select(['Key', 'Lyric', 'Genre']).withColumnRenamed('Lyric','text')\n",
        "\n",
        "# train, test = trainDataset.randomSplit(weights=[0.5, 0.5], seed=123)\n",
        "song_data = song_data.limit(999)\n",
        "print(song_data.count())\n",
        "\n",
        "song_data = song_data.filter(song_data.text != '')\n",
        "print(song_data.count())\n",
        "song_data = song_data.filter((song_data.Genre == 'Rock') | (song_data.Genre == 'Hip Hop') | (song_data.Genre == 'Pop')  )\n",
        "# song_data = song_data.filter(song_data.Genre != '')\n",
        "print(song_data.count())\n",
        "# song_data = song_data.filter(song_data.text != ' ')\n",
        "# print(song_data.count())\n",
        "# song_data = song_data.filter(song_data.Genre != ' ')\n",
        "# print(song_data.count())\n",
        "song_data.na.drop(subset=[\"text\"])\n",
        "print(song_data.count())\n",
        "song_data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n",
            "999\n",
            "915\n",
            "915\n",
            "+--------------------+--------------------+-------+\n",
            "|                 Key|                text|  Genre|\n",
            "+--------------------+--------------------+-------+\n",
            "|10000 maniacs_Mor...|I could feel at t...|   Rock|\n",
            "|10000 maniacs_Bec...|Take me now, baby...|   Rock|\n",
            "|jamiroquai_Rock D...|And it's coming a...|    Pop|\n",
            "|10000 maniacs_The...|These are. These ...|   Rock|\n",
            "|10000 maniacs_Eve...|Trudging slowly o...|   Rock|\n",
            "|10000 maniacs_Don...|Don't talk, I wil...|   Rock|\n",
            "|black veil brides...|Have we begun to ...|   Rock|\n",
            "|lynyrd skynyrd_I ...|Ain't no need to ...|   Rock|\n",
            "|10000 maniacs_Acr...|Well they left th...|   Rock|\n",
            "|10000 maniacs_Pla...|[ music: Dennis D...|   Rock|\n",
            "|10000 maniacs_Rai...|On bended kneeI'v...|   Rock|\n",
            "|twista_Back 2 School|[Tung Twista]. ba...|Hip Hop|\n",
            "|10000 maniacs_Ant...|For whom do the b...|   Rock|\n",
            "|10000 maniacs_All...|She walks alone o...|   Rock|\n",
            "|10000 maniacs_Bac...|Jenny. Jenny you ...|   Rock|\n",
            "|cyndi lauper_True...|You with the sad ...|    Pop|\n",
            "|10000 maniacs_A R...|You were looking ...|   Rock|\n",
            "|rick astley_She M...|She makes me more...|    Pop|\n",
            "|steve earle_Nothi...|I'm the keeper of...|   Rock|\n",
            "|10000 maniacs_Mad...|\"the legs of Madd...|   Rock|\n",
            "+--------------------+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyXMOyWZP4Xb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b4f395f-9245-4b29-be01-31ba418e6368"
      },
      "source": [
        "print(type(song_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPdwWmc3iPz",
        "colab_type": "text"
      },
      "source": [
        "# Package Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeBx3-P6P6Q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "792cd635-3357-41a6-cc46-0824d453abe4"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from pyspark.sql import functions as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_L4yv9Qc_-",
        "colab_type": "text"
      },
      "source": [
        "# Vocab Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiHyoUe_33ci",
        "colab_type": "text"
      },
      "source": [
        "Basic Data cleaning for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atU2csZwRIJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "6b34fa0a-49ba-4419-c21c-b644e75486b6"
      },
      "source": [
        "assembler = DocumentAssembler().setInputCol('text').setOutputCol('document')\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('token')#.setTargetPattern('/\\b(\\?You were looking)\\b/')#.setExceptionsPath('/content/MyDrive/SparkNLP/entities.txt')\n",
        "\n",
        "spell_chk = NorvigSweetingModel().pretrained().setInputCols(['token']).setOutputCol('corrected')\n",
        "\n",
        "lemmatizer = LemmatizerModel().pretrained().setInputCols(['corrected']).setOutputCol('lemma')\n",
        "\n",
        "normalizer = Normalizer().setInputCols(['lemma']).setOutputCol('normalized').setLowercase(True)\n",
        "\n",
        "stop_wrd = list( stopwords.words('english'))\n",
        "\n",
        "stop_words_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False).setStopWords(stop_wrd)\n",
        "\n",
        "finisher = Finisher().setInputCols(['cleanTokens']).setOutputCols(['cleanTokens']).setOutputAsArray(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spellcheck_norvig download started this may take some time.\n",
            "Approximate size to download 4.2 MB\n",
            "[OK!]\n",
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfOlqRGP4qnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "c28248a5-7858-43dc-c7aa-a90265999cee"
      },
      "source": [
        "pipeline_bow = Pipeline().setStages([\n",
        "    assembler, tokenizer, spell_chk , \n",
        "    lemmatizer, normalizer,stop_words_cleaner,finisher\n",
        "])\n",
        "model_trans_tfIdf = pipeline_bow.fit(song_data)\n",
        "model_trans_tfIdf =  model_trans_tfIdf.transform(song_data)\n",
        "model_trans_tfIdf.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------+--------------------+\n",
            "|                 Key|                text|  Genre|         cleanTokens|\n",
            "+--------------------+--------------------+-------+--------------------+\n",
            "|10000 maniacs_Mor...|I could feel at t...|   Rock|[could, feel, tim...|\n",
            "|10000 maniacs_Bec...|Take me now, baby...|   Rock|[take, baby, hold...|\n",
            "|jamiroquai_Rock D...|And it's coming a...|    Pop|[come, baby, yeah...|\n",
            "|10000 maniacs_The...|These are. These ...|   Rock|[day, youll, reme...|\n",
            "|10000 maniacs_Eve...|Trudging slowly o...|   Rock|[grudge, slowly, ...|\n",
            "|10000 maniacs_Don...|Don't talk, I wil...|   Rock|[donut, talk, lis...|\n",
            "|black veil brides...|Have we begun to ...|   Rock|[begin, drift, aw...|\n",
            "|lynyrd skynyrd_I ...|Ain't no need to ...|   Rock|[aint, need, worr...|\n",
            "|10000 maniacs_Acr...|Well they left th...|   Rock|[well, leave, mor...|\n",
            "|10000 maniacs_Pla...|[ music: Dennis D...|   Rock|[music, dennis, d...|\n",
            "|10000 maniacs_Rai...|On bended kneeI'v...|   Rock|[bend, kneeive, l...|\n",
            "|twista_Back 2 School|[Tung Twista]. ba...|Hip Hop|[tung, twista, ba...|\n",
            "|10000 maniacs_Ant...|For whom do the b...|   Rock|[bell, toll, sent...|\n",
            "|10000 maniacs_All...|She walks alone o...|   Rock|[walk, alone, bri...|\n",
            "|10000 maniacs_Bac...|Jenny. Jenny you ...|   Rock|[jenny, jenny, do...|\n",
            "|cyndi lauper_True...|You with the sad ...|    Pop|[sad, eye, donut,...|\n",
            "|10000 maniacs_A R...|You were looking ...|   Rock|[look, away, west...|\n",
            "|rick astley_She M...|She makes me more...|    Pop|[make, could, eve...|\n",
            "|steve earle_Nothi...|I'm the keeper of...|   Rock|[im, keeper, hear...|\n",
            "|10000 maniacs_Mad...|\"the legs of Madd...|   Rock|[leg, maddox, kit...|\n",
            "+--------------------+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EZOEwAdQheP",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hFRMcLVeJTt",
        "colab_type": "text"
      },
      "source": [
        "## BoW Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NZYQS9OPvsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "8081177f-64b3-4aa9-ebba-3f3ab86c8049"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "\n",
        "count_vectorizer = CountVectorizer(inputCol='cleanTokens', outputCol='tf', minDF=10)\n",
        "cvmodel = count_vectorizer.fit(model_trans_tfIdf)\n",
        "idf = IDF(inputCol='tf', outputCol='features', minDocFreq=10)\n",
        "# finisher_idf = Finisher().setInputCols(['features']).setOutputCols(['tfidf']) finisher is for sparkNLP not spark ML\n",
        "bow_pipeline = Pipeline().setStages([count_vectorizer, idf])\n",
        "bow_pipeline = bow_pipeline.fit(model_trans_tfIdf)\n",
        "\n",
        "bows = bow_pipeline.transform(model_trans_tfIdf)\n",
        "bows.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------+--------------------+--------------------+--------------------+\n",
            "|                 Key|                text|  Genre|         cleanTokens|                  tf|            features|\n",
            "+--------------------+--------------------+-------+--------------------+--------------------+--------------------+\n",
            "|10000 maniacs_Mor...|I could feel at t...|   Rock|[could, feel, tim...|(881,[0,1,3,4,8,1...|(881,[0,1,3,4,8,1...|\n",
            "|10000 maniacs_Bec...|Take me now, baby...|   Rock|[take, baby, hold...|(881,[0,6,7,10,12...|(881,[0,6,7,10,12...|\n",
            "|jamiroquai_Rock D...|And it's coming a...|    Pop|[come, baby, yeah...|(881,[0,1,2,3,4,5...|(881,[0,1,2,3,4,5...|\n",
            "|10000 maniacs_The...|These are. These ...|   Rock|[day, youll, reme...|(881,[1,9,11,15,1...|(881,[1,9,11,15,1...|\n",
            "|10000 maniacs_Eve...|Trudging slowly o...|   Rock|[grudge, slowly, ...|(881,[4,12,24,80,...|(881,[4,12,24,80,...|\n",
            "|10000 maniacs_Don...|Don't talk, I wil...|   Rock|[donut, talk, lis...|(881,[0,1,3,5,9,1...|(881,[0,1,3,5,9,1...|\n",
            "|black veil brides...|Have we begun to ...|   Rock|[begin, drift, aw...|(881,[2,3,4,6,8,1...|(881,[2,3,4,6,8,1...|\n",
            "|lynyrd skynyrd_I ...|Ain't no need to ...|   Rock|[aint, need, worr...|(881,[0,1,2,3,6,7...|(881,[0,1,2,3,6,7...|\n",
            "|10000 maniacs_Acr...|Well they left th...|   Rock|[well, leave, mor...|(881,[3,7,17,20,2...|(881,[3,7,17,20,2...|\n",
            "|10000 maniacs_Pla...|[ music: Dennis D...|   Rock|[music, dennis, d...|(881,[9,11,23,44,...|(881,[9,11,23,44,...|\n",
            "|10000 maniacs_Rai...|On bended kneeI'v...|   Rock|[bend, kneeive, l...|(881,[4,7,12,22,3...|(881,[4,7,12,22,3...|\n",
            "|twista_Back 2 School|[Tung Twista]. ba...|Hip Hop|[tung, twista, ba...|(881,[0,1,2,3,4,7...|(881,[0,1,2,3,4,7...|\n",
            "|10000 maniacs_Ant...|For whom do the b...|   Rock|[bell, toll, sent...|(881,[4,9,11,16,1...|(881,[4,9,11,16,1...|\n",
            "|10000 maniacs_All...|She walks alone o...|   Rock|[walk, alone, bri...|(881,[1,3,4,7,9,1...|(881,[1,3,4,7,9,1...|\n",
            "|10000 maniacs_Bac...|Jenny. Jenny you ...|   Rock|[jenny, jenny, do...|(881,[0,1,2,5,8,9...|(881,[0,1,2,5,8,9...|\n",
            "|cyndi lauper_True...|You with the sad ...|    Pop|[sad, eye, donut,...|(881,[1,4,5,6,7,1...|(881,[1,4,5,6,7,1...|\n",
            "|10000 maniacs_A R...|You were looking ...|   Rock|[look, away, west...|(881,[0,1,8,11,12...|(881,[0,1,8,11,12...|\n",
            "|rick astley_She M...|She makes me more...|    Pop|[make, could, eve...|(881,[0,4,7,9,11,...|(881,[0,4,7,9,11,...|\n",
            "|steve earle_Nothi...|I'm the keeper of...|   Rock|[im, keeper, hear...|(881,[0,5,8,10,19...|(881,[0,5,8,10,19...|\n",
            "|10000 maniacs_Mad...|\"the legs of Madd...|   Rock|[leg, maddox, kit...|(881,[3,11,16,21,...|(881,[3,11,16,21,...|\n",
            "+--------------------+--------------------+-------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzpsGf77KkNn",
        "colab_type": "text"
      },
      "source": [
        "## Approach 1: Top 3 > Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlGaSbozA9O4",
        "colab_type": "text"
      },
      "source": [
        "Vectorization create vector column object and we need to retrieve values and indices accordingly from it unlike python array/list\n",
        "This whole para shows how we can retrieve indices and ranking from feature column in various ways by applying UDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT5AubDo2BEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "50008fd4-0083-45b9-fdd8-45bbf7816fb0"
      },
      "source": [
        "from collections import Counter\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "from pyspark.sql.types import *\n",
        "# retrieve column with dict indices>TFIDF_Rank \n",
        "# @udf(\"map<long, double>\")\n",
        "# def vector_as_map(v):\n",
        "#    if isinstance(v, SparseVector):\n",
        "#        return dict(zip(v.indices.tolist(), v.values.tolist()))\n",
        "       \n",
        "#    elif isinstance(v, DenseVector):\n",
        "#       print(\"x\")\n",
        "#       return dict(zip(range(len(v)), v.values.tolist()))\n",
        "\n",
        "# bows_wrdlevel = bows.select(\"cleanTokens\", vector_as_map(\"features\").alias(\"rating\"))\n",
        "# bows_wrdlevel.show(truncate = False)\n",
        "\n",
        "# retrieve Columns with dict of only prominent word (based on rating)\n",
        " \n",
        "# schema = ArrayType(StructType([\n",
        "#     StructField(\"ind\", IntegerType(), False),\n",
        "#     StructField(\"rat\", DoubleType(), False)\n",
        "# ]))\n",
        "\n",
        "# @udf(schema)\n",
        "# def vector_as_map(v):\n",
        "#    if isinstance(v, SparseVector):\n",
        "#       K = Counter(dict(zip(v.indices.tolist(), v.values.tolist())))\n",
        "#       return K.most_common(3)\n",
        "#    elif isinstance(v, DenseVector):\n",
        "#       L =Counter(dict(zip(range(len(v)), v.values.tolist())))\n",
        "#       return L.most_common(3) \n",
        "\n",
        "# bows_wrdlevel = bows.select(\"cleanTokens\", vector_as_map(\"features\").alias(\"rating\"))\n",
        "# bows_wrdlevel.show(truncate = False)\n",
        "\n",
        "# getting top 3 indices without retaining their rank\n",
        "\n",
        "# schema = ArrayType(StructType([\n",
        "#     StructField(\"ind\", IntegerType(), False),\n",
        "#     StructField(\"rat\", DoubleType(), False)\n",
        "# ]))\n",
        "\n",
        "@udf(ArrayType(IntegerType()))\n",
        "def vector_as_map(v):\n",
        "   if isinstance(v, SparseVector):\n",
        "      K = dict(zip(v.indices.tolist(), v.values.tolist()))\n",
        "      top3 = sorted(K, key=K.get, reverse=True)[:3]\n",
        "      return top3\n",
        "   elif isinstance(v, DenseVector):\n",
        "      L =dict(zip(range(len(v)), v.values.tolist()))\n",
        "      top3 = sorted(L, key=L.get, reverse=True)[:3]\n",
        "      return top3 \n",
        "\n",
        "bows_wrdlevel = bows.select(\"Key\",'Genre', vector_as_map(\"features\").alias(\"top_ind\"))\n",
        "bows_wrdlevel.na.drop(subset=[\"top_ind\"])\n",
        "bows_wrdlevel.show(truncate = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+---------------+\n",
            "|                 Key|  Genre|        top_ind|\n",
            "+--------------------+-------+---------------+\n",
            "|10000 maniacs_Mor...|   Rock|  [70, 57, 718]|\n",
            "|10000 maniacs_Bec...|   Rock| [320, 54, 424]|\n",
            "|jamiroquai_Rock D...|    Pop|[609, 588, 249]|\n",
            "|10000 maniacs_The...|   Rock| [61, 173, 555]|\n",
            "|10000 maniacs_Eve...|   Rock|[413, 768, 682]|\n",
            "|10000 maniacs_Don...|   Rock| [107, 43, 828]|\n",
            "|black veil brides...|   Rock|  [6, 147, 337]|\n",
            "|lynyrd skynyrd_I ...|   Rock|    [41, 6, 33]|\n",
            "|10000 maniacs_Acr...|   Rock|[298, 490, 226]|\n",
            "|10000 maniacs_Pla...|   Rock|[832, 195, 716]|\n",
            "|10000 maniacs_Rai...|   Rock| [660, 147, 53]|\n",
            "|twista_Back 2 School|Hip Hop|[378, 188, 249]|\n",
            "|10000 maniacs_Ant...|   Rock|[761, 227, 744]|\n",
            "|10000 maniacs_All...|   Rock| [155, 65, 461]|\n",
            "|10000 maniacs_Bac...|   Rock|[353, 692, 370]|\n",
            "|cyndi lauper_True...|    Pop|[445, 173, 223]|\n",
            "|10000 maniacs_A R...|   Rock| [63, 251, 807]|\n",
            "|rick astley_She M...|    Pop| [30, 156, 203]|\n",
            "|steve earle_Nothi...|   Rock|  [371, 130, 0]|\n",
            "|10000 maniacs_Mad...|   Rock|[697, 368, 734]|\n",
            "+--------------------+-------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2hrOM8MSdiM",
        "colab_type": "text"
      },
      "source": [
        "Convert indices to word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnp0qvaJSc3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from operator import itemgetter\n",
        "\n",
        "vocab = cvmodel.vocabulary\n",
        "# udf_to_words = udf(get_words, ArrayType(StringType()))\n",
        "# use @udf if you have used that pattern, throughout your pynb\n",
        "@udf(ArrayType(StringType()))\n",
        "def get_words(token_in):\n",
        "    return itemgetter(*token_in)(vocab)\n",
        "    # return map(vocab.__getitem__, token_in)\n",
        "    # return vocab[token_in].tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQjqJsLOolsf",
        "colab_type": "text"
      },
      "source": [
        "This gives us top 3 word for each doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs754MWD5WeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "323a2ad3-dc53-49a1-9737-b8d07b165714"
      },
      "source": [
        "bow_top3_wrd = bows_wrdlevel.select('Key','Genre',get_words(F.col('top_ind')).alias('word'))\n",
        "bow_top3_wrd.na.drop(subset=[\"word\"])\n",
        "bow_top3_wrd.show()\n",
        "# bow_top5_wrd.schema\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+--------------------+\n",
            "|                 Key|  Genre|                word|\n",
            "+--------------------+-------+--------------------+\n",
            "|10000 maniacs_Mor...|   Rock|[theres, nothing,...|\n",
            "|10000 maniacs_Bec...|   Rock|[belong, night, l...|\n",
            "|jamiroquai_Rock D...|    Pop|[coming, page, rock]|\n",
            "|10000 maniacs_The...|   Rock|[youll, true, bless]|\n",
            "|10000 maniacs_Eve...|   Rock|[everyday, pack, ...|\n",
            "|10000 maniacs_Don...|   Rock|[really, keep, th...|\n",
            "|black veil brides...|   Rock| [oh, save, mistake]|\n",
            "|lynyrd skynyrd_I ...|   Rock|    [baby, oh, need]|\n",
            "|10000 maniacs_Acr...|   Rock|[anymore, trouble...|\n",
            "|10000 maniacs_Pla...|   Rock|[brain, truth, st...|\n",
            "|10000 maniacs_Rai...|   Rock|[theyve, save, wo...|\n",
            "|twista_Back 2 School|Hip Hop| [school, use, rock]|\n",
            "|10000 maniacs_Ant...|   Rock|[heat, fire, surr...|\n",
            "|10000 maniacs_All...|   Rock|[forever, hold, s...|\n",
            "|10000 maniacs_Bac...|   Rock|[angel, style, shes]|\n",
            "|cyndi lauper_True...|    Pop|[mama, true, afraid]|\n",
            "|10000 maniacs_A R...|   Rock|   [wait, room, dry]|\n",
            "|rick astley_She M...|    Pop|[give, maybe, alive]|\n",
            "|steve earle_Nothi...|   Rock|[shoot, without, im]|\n",
            "|10000 maniacs_Mad...|   Rock|  [table, hes, shut]|\n",
            "+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NosRdofjkzdS",
        "colab_type": "text"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rATK2hvD4qW",
        "colab_type": "text"
      },
      "source": [
        "embedding works on sentence as it tries to find out the relationship between words too and not just their count/occurence. Because of this we have Approach 2 where we are creating embedding first and then doing picking embeddings for top 3 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVUyMvB_6gCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "49eccda4-3904-4401-abbf-2c2b0aa6c1df"
      },
      "source": [
        "#convert list to sentence\n",
        "@udf( StringType())\n",
        "def get_sentence(word_in):\n",
        "    return \" \".join(str(item) for item in word_in)\n",
        "\n",
        "# udf_to_doc = udf(get_sentence, StringType())\n",
        "\n",
        "bow_top_embd = bow_top3_wrd.select('Key','Genre', get_sentence(F.col('word')).alias('doc'))\n",
        "# bow_top_embd = bow_top_embd.filter((F.col('doc') != '') | F.col('doc') != ' ')\n",
        "bow_top_embd.na.drop(subset=[\"doc\"])\n",
        "from pyspark.sql.functions import trim\n",
        "bow_top_embd = bow_top_embd.withColumn(\"doc\", trim(bow_top_embd.doc))\n",
        "# bow_top_embd.filter(!(F.col('doc') == ' ')).show()\n",
        "bow_top_embd.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+--------------------+\n",
            "|                 Key|  Genre|                 doc|\n",
            "+--------------------+-------+--------------------+\n",
            "|10000 maniacs_Mor...|   Rock|nothing theres shout|\n",
            "|10000 maniacs_Bec...|   Rock|  belong night lover|\n",
            "|jamiroquai_Rock D...|    Pop|    coming page rock|\n",
            "|10000 maniacs_The...|   Rock|    youll true bless|\n",
            "|10000 maniacs_Eve...|   Rock|everyday pack silent|\n",
            "|10000 maniacs_Don...|   Rock|   really keep three|\n",
            "|black veil brides...|   Rock|     oh save mistake|\n",
            "|lynyrd skynyrd_I ...|   Rock|        baby oh need|\n",
            "|10000 maniacs_Acr...|   Rock|anymore trouble part|\n",
            "|10000 maniacs_Pla...|   Rock|brain truth strength|\n",
            "|10000 maniacs_Rai...|   Rock|   theyve save would|\n",
            "|twista_Back 2 School|Hip Hop|     school use rock|\n",
            "|10000 maniacs_Ant...|   Rock|  heat fire surround|\n",
            "|10000 maniacs_All...|   Rock|  forever hold sorry|\n",
            "|10000 maniacs_Bac...|   Rock|    angel style shes|\n",
            "|cyndi lauper_True...|    Pop|    mama true afraid|\n",
            "|10000 maniacs_A R...|   Rock|       wait room dry|\n",
            "|rick astley_She M...|    Pop|    give maybe alive|\n",
            "|steve earle_Nothi...|   Rock|    shoot without im|\n",
            "|10000 maniacs_Mad...|   Rock|      table hes shut|\n",
            "+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-xGmdch_iZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9c23db91-3243-4d9c-a197-a942226d02ca"
      },
      "source": [
        "bow_top_embd.printSchema()\n",
        "bow_top_embd.select('Genre').distinct().collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Key: string (nullable = true)\n",
            " |-- Genre: string (nullable = true)\n",
            " |-- doc: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Genre='Rock'), Row(Genre='Pop'), Row(Genre='Hip Hop')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcowO4ga4UJ",
        "colab_type": "text"
      },
      "source": [
        "Embedding output here will be list of list 3*100 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwKs9Ajzl0lM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "29986a94-963d-46ac-a224-da6f04795cf9"
      },
      "source": [
        "assembler = DocumentAssembler().setInputCol('doc').setOutputCol(\"document\").setCleanupMode(\"shrink\")\n",
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('token')\n",
        "embed = WordEmbeddingsModel.pretrained().setInputCols(['document',\"token\"]).setOutputCol(\"embeddings\")\n",
        "embeddings_finisher = EmbeddingsFinisher().setInputCols([\"embeddings\"]).setOutputCols([\"embeddings\"]).setOutputAsVector(True).setCleanAnnotations(False)\n",
        "finisher = Finisher().setInputCols(['token']).setOutputCols(['token']).setOutputAsArray(True)\n",
        "# vec_assembler = VectorAssembler().setInputCols(['embeddings']).setOutputCol('features')\n",
        "pipeline_uni = Pipeline().setStages([\n",
        "    assembler, tokenizer,embed ,embeddings_finisher, finisher\n",
        "])\n",
        "model = pipeline_uni.fit(bow_top_embd)\n",
        "model_trans =  model.transform(bow_top_embd)#.persist()\n",
        "# model_trans.select('embeddings').show(truncate = False)\n",
        "model_trans.show(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|                 Key|Genre|                 doc|               token|          embeddings|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|10000 maniacs_Mor...| Rock|nothing theres shout|[nothing, theres,...|[[0.1090399995446...|\n",
            "|10000 maniacs_Bec...| Rock|  belong night lover|[belong, night, l...|[[0.0577719993889...|\n",
            "|jamiroquai_Rock D...|  Pop|    coming page rock|[coming, page, rock]|[[0.0667089968919...|\n",
            "|10000 maniacs_The...| Rock|    youll true bless|[youll, true, bless]|[[-0.556039988994...|\n",
            "|10000 maniacs_Eve...| Rock|everyday pack silent|[everyday, pack, ...|[[-0.350219994783...|\n",
            "|10000 maniacs_Don...| Rock|   really keep three|[really, keep, th...|[[-0.054349001497...|\n",
            "|black veil brides...| Rock|     oh save mistake| [oh, save, mistake]|[[-0.666649997234...|\n",
            "|lynyrd skynyrd_I ...| Rock|        baby oh need|    [baby, oh, need]|[[0.4974699914455...|\n",
            "|10000 maniacs_Acr...| Rock|anymore trouble part|[anymore, trouble...|[[0.3865799903869...|\n",
            "|10000 maniacs_Pla...| Rock|brain truth strength|[brain, truth, st...|[[-0.102439999580...|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faXVlQDqa7_o",
        "colab_type": "text"
      },
      "source": [
        "String Indexer to change string categories to label for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZg00gXxh9Td",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "3085c6bf-5b26-41a8-b856-146624fc5886"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"Genre\", outputCol = \"label\" ,)\n",
        "pipeline = Pipeline(stages=[ label_stringIdx])\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(model_trans)\n",
        "dataset = pipelineFit.transform(model_trans)\n",
        "selectedCols = ['token','embeddings','label']\n",
        "dataset = dataset.select(selectedCols)\n",
        "dataset.show(5)\n",
        "# dataset.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|               token|          embeddings|label|\n",
            "+--------------------+--------------------+-----+\n",
            "|[theres, nothing,...|[[-0.123620003461...|  0.0|\n",
            "|[belong, night, l...|[[0.0577719993889...|  0.0|\n",
            "|[coming, page, rock]|[[0.0667089968919...|  1.0|\n",
            "|[youll, true, bless]|[[-0.556039988994...|  0.0|\n",
            "|[everyday, pack, ...|[[-0.350219994783...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_LJcEsc9bH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f1b9975-625a-4906-f44b-234c24eb3da4"
      },
      "source": [
        "dataset.select('label').distinct().collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(label=0.0), Row(label=1.0), Row(label=2.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQv0R9kSCgDn",
        "colab_type": "text"
      },
      "source": [
        "Spark ML takes vector as input so need to convert our embedding output to vector. We are going to use dense vector here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEyiQFt5z7mr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "3898a24f-0f23-4cf2-e7eb-5dc37be47d93"
      },
      "source": [
        "import itertools\n",
        "import functools\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "@udf(VectorUDT())\n",
        "# @udf(StringType())\n",
        "def get_Flatvector(embd_vec):\n",
        "  # embd_vec = list(embd_vec)\n",
        "  merged = list(itertools.chain(*embd_vec))\n",
        "  # merged = functools.reduce(operator.iconcat, embd_vec, [])\n",
        "  mer_floats = [float(np_float) for np_float in merged]\n",
        "  ver = Vectors.dense(mer_floats)\n",
        "  return ver\n",
        "  # return str(type(mer_floats[0]))\n",
        "  # return embd_vec[0].tolist()\n",
        "\n",
        "dataset_clsf = dataset.select('token',get_Flatvector(F.col('embeddings')).alias('features'),'label')\n",
        "dataset_clsf.na.drop(subset=[\"features\"])\n",
        "# dataset_clsf = dataset_clsf.where(\"'label' != ''\").where(\"'label' != ' '\")\n",
        "# dataset_clsf = dataset_clsf.filter(dataset_clsf.label == 0.0)\n",
        "dataset_clsf.show(truncate = True)\n",
        "\n",
        "# dataset.select(\"label\",flatten(\"embeddings\")).show(false)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|               token|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "|[theres, nothing,...|[-0.1236200034618...|  0.0|\n",
            "|[belong, night, l...|[0.05777199938893...|  0.0|\n",
            "|[coming, page, rock]|[0.06670899689197...|  1.0|\n",
            "|[youll, true, bless]|[-0.5560399889945...|  0.0|\n",
            "|[everyday, pack, ...|[-0.3502199947834...|  0.0|\n",
            "|[really, keep, th...|[-0.0543490014970...|  0.0|\n",
            "| [oh, save, mistake]|[-0.6666499972343...|  0.0|\n",
            "|    [baby, oh, need]|[0.49746999144554...|  0.0|\n",
            "|[anymore, trouble...|[0.38657999038696...|  0.0|\n",
            "|[brain, truth, st...|[-0.1024399995803...|  0.0|\n",
            "|[theyve, save, wo...|[0.0,0.0,0.0,0.0,...|  0.0|\n",
            "| [school, use, rock]|[0.80335998535156...|  2.0|\n",
            "|[heat, fire, surr...|[-0.8194400072097...|  0.0|\n",
            "|[forever, hold, s...|[-0.0296199992299...|  0.0|\n",
            "|[angel, style, shes]|[0.34995999932289...|  0.0|\n",
            "|[mama, true, afraid]|[0.27950999140739...|  1.0|\n",
            "|   [wait, room, dry]|[-0.1498499959707...|  0.0|\n",
            "|[give, maybe, alive]|[-0.0376080013811...|  1.0|\n",
            "|[shoot, without, im]|[-0.2886199951171...|  0.0|\n",
            "|  [table, hes, shut]|[-0.6145399808883...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZHM_xzVDLHN",
        "colab_type": "text"
      },
      "source": [
        "We need to ensure that the size of feature is 300. group by count of feature to check and remove unwanted rows. This is probably because TFIDF gave same rank to multiple words and we have more than 3 in some cases. in our case it was just one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPzEK8Q0ODxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "4dc47a68-8837-4252-d446-6e5dfa1a7750"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "import itertools\n",
        "import functools\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "# @udf(VectorUDT())\n",
        "@udf(IntegerType())\n",
        "def get_Flatvector_len(embd_vec):\n",
        "  # embd_vec = list(embd_vec)\n",
        "  # merged = list(itertools.chain(*embd_vec))\n",
        "  # merged = functools.reduce(operator.iconcat, embd_vec, [])\n",
        "  # mer_floats = [float(np_float) for np_float in merged]\n",
        "  vec =Vectors.dense(embd_vec)\n",
        "  return (len(vec))\n",
        "  # return str(type(mer_floats[0]))\n",
        "  # return embd_vec[0].tolist()\n",
        "\n",
        "dataset_clsf_300 = dataset_clsf.select('token','features',get_Flatvector_len(F.col('features')).alias('feat_cnt'),'label')\n",
        "dataset_clsf_300.na.drop(subset=[\"feat_cnt\"])\n",
        "dataset_clsf_300.na.drop(subset=[\"features\"])\n",
        "dataset_clsf_300.na.drop(subset=[\"label\"])\n",
        "# dataset_clsf.show(truncate = True)\n",
        "# dataset_clsf = dataset_clsf.groupBy(\"features\").agg(F.count('features'))\n",
        "dataset_clsf_300 = dataset_clsf_300.filter((dataset_clsf_300.feat_cnt == 300) | (dataset_clsf_300.label != ' ') ).select('token','features','label')\n",
        "# dataset_clsf_300 = dataset_clsf_300.filter(dataset_clsf_300.label != ' ').select('features','label')\n",
        "# dataset_clsf_300 = dataset_clsf_300.filter(F.col('features') != ' ')\n",
        "# dataset_clsf_300.show(dataset_clsf_300.count() )\n",
        "# dataset_clsf_300.repartition(1).count()\n",
        "dataset_clsf_300.show()\n",
        "# dataset_clsf_300.count()\n",
        "# dataset_clsf_300.groupBy(\"feat_cnt\").agg(F.count('feat_cnt')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|               token|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "|[theres, nothing,...|[-0.1236200034618...|  0.0|\n",
            "|[belong, night, l...|[0.05777199938893...|  0.0|\n",
            "|[coming, page, rock]|[0.06670899689197...|  1.0|\n",
            "|[youll, true, bless]|[-0.5560399889945...|  0.0|\n",
            "|[everyday, pack, ...|[-0.3502199947834...|  0.0|\n",
            "|[really, keep, th...|[-0.0543490014970...|  0.0|\n",
            "| [oh, save, mistake]|[-0.6666499972343...|  0.0|\n",
            "|    [baby, oh, need]|[0.49746999144554...|  0.0|\n",
            "|[anymore, trouble...|[0.38657999038696...|  0.0|\n",
            "|[brain, truth, st...|[-0.1024399995803...|  0.0|\n",
            "|[theyve, save, wo...|[0.0,0.0,0.0,0.0,...|  0.0|\n",
            "| [school, use, rock]|[0.80335998535156...|  2.0|\n",
            "|[heat, fire, surr...|[-0.8194400072097...|  0.0|\n",
            "|[forever, hold, s...|[-0.0296199992299...|  0.0|\n",
            "|[angel, style, shes]|[0.34995999932289...|  0.0|\n",
            "|[mama, true, afraid]|[0.27950999140739...|  1.0|\n",
            "|   [wait, room, dry]|[-0.1498499959707...|  0.0|\n",
            "|[give, maybe, alive]|[-0.0376080013811...|  1.0|\n",
            "|[shoot, without, im]|[-0.2886199951171...|  0.0|\n",
            "|  [table, hes, shut]|[-0.6145399808883...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU2wwXTqjHkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "66b2725c-880c-45b2-e552-542f6a638b0c"
      },
      "source": [
        "# set seed for reproducibility\n",
        "# train, test = dataset_clsf_300.randomSplit(weights=[0.6, 0.4], seed=100)\n",
        "# dataset_clsf_300 = dataset_clsf_300.where(\"'features' != ''\")\n",
        "(trainingData, testData) = dataset_clsf_300.select('token','features','label').randomSplit([0.7, 0.3], seed = 100)\n",
        "# print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "# print(\"Test Dataset Count: \" + str(testData.count()))\n",
        "# train, test = dataset_clsf_300.randomSplit([0.6, 0.4], seed=100)\n",
        "trainingData.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|               token|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "|      [act, eye, uh]|[0.24027000367641...|  0.0|\n",
            "|[afraid, perfect,...|[0.20393000543117...|  0.0|\n",
            "|[afraid, perfect,...|[0.20393000543117...|  0.0|\n",
            "|[afraid, sweet, p...|[0.20393000543117...|  0.0|\n",
            "|   [ah, baby, steal]|[-1.4133000373840...|  1.0|\n",
            "|[ah, believe, fri...|[-1.4133000373840...|  0.0|\n",
            "| [alone, goin, help]|[0.38927999138832...|  0.0|\n",
            "|[alone, wrong, ni...|[0.38927999138832...|  0.0|\n",
            "|[already, round, ...|[0.13563999533653...|  0.0|\n",
            "|[already, round, ...|[0.13563999533653...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUZykjGRHitl",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAxiD5Y6_yMX",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV8uISUrjS-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "df4ce789-aa47-4a6f-c4a2-9cdcd6f4e216"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label',maxIter=20, regParam=0.3, elasticNetParam=0   )\n",
        "lrModel = lr.fit(trainingData )\n",
        "predictions = lrModel.transform(testData)\n",
        "predictions.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
            "|               token|            features|label|       rawPrediction|         probability|prediction|\n",
            "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
            "| [ago, rock, become]|[0.54666000604629...|  0.0|[1.41761643875231...|[0.79741206092989...|       0.0|\n",
            "|   [ah, yeah, crazy]|[-1.4133000373840...|  1.0|[-0.1186564800670...|[0.24700552033410...|       1.0|\n",
            "|[aint, call, always]|[-0.1444900035858...|  1.0|[1.66194859571792...|[0.83348033887642...|       0.0|\n",
            "|[alive, night, ever]|[-0.0055056000128...|  1.0|[1.50112892351291...|[0.80985501917549...|       0.0|\n",
            "|[already, young, ...|[0.13563999533653...|  0.0|[1.28606549779604...|[0.75176838242710...|       0.0|\n",
            "|  [alright, oh, try]|[0.34696000814437...|  1.0|[1.30015038665708...|[0.77115126204828...|       0.0|\n",
            "|[angel, style, shes]|[0.34995999932289...|  0.0|[1.67267610797735...|[0.82726193161393...|       0.0|\n",
            "| [angel, today, gun]|[0.34995999932289...|  0.0|[1.45533061587012...|[0.78274042762116...|       0.0|\n",
            "|[anymore, trouble...|[0.38657999038696...|  0.0|[1.94382655069015...|[0.85622207318998...|       0.0|\n",
            "|[anymore, trouble...|[0.38657999038696...|  0.0|[1.94382655069015...|[0.85622207318998...|       0.0|\n",
            "|    [baby, oh, need]|[0.49746999144554...|  0.0|[1.32920129829678...|[0.76951463006016...|       0.0|\n",
            "|   [baby, top, home]|[0.49746999144554...|  1.0|[1.16868913101335...|[0.58213944966820...|       0.0|\n",
            "|[baby, without, d...|[0.49746999144554...|  0.0|[1.69711544022955...|[0.75172016807629...|       0.0|\n",
            "|[ball, shoulder, ...|[-0.0365669988095...|  0.0|[1.11691901131013...|[0.68746424103510...|       0.0|\n",
            "| [beast, break, son]|[0.01666799932718...|  0.0|[1.82487505980117...|[0.87246776270177...|       0.0|\n",
            "|   [beg, id, misery]|[-0.5893800258636...|  0.0|[1.28354985563475...|[0.64803839692784...|       0.0|\n",
            "|[believe, tell, lip]|[0.11221999675035...|  0.0|[1.41785560912943...|[0.75585586462292...|       0.0|\n",
            "|[belong, night, l...|[0.05777199938893...|  0.0|[1.84725264155818...|[0.86335479231514...|       0.0|\n",
            "|   [bitch, last, ah]|[0.33096998929977...|  2.0|[1.37792096366267...|[0.63005728044327...|       0.0|\n",
            "|   [black, end, fun]|[-0.0574459992349...|  0.0|[1.44446352943006...|[0.75541221768381...|       0.0|\n",
            "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyfuQBQXWGU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a129376-2e1a-4a66-b864-04ceadb84964"
      },
      "source": [
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)\n",
        "# predictions.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.735822417115046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o-XJS6wl-aa",
        "colab_type": "text"
      },
      "source": [
        "#### Top Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQRe_GDEmDif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "3e4d532f-89aa-4217-ef64-2bc4dcbdc793"
      },
      "source": [
        "top_wrd_df = predictions.filter(predictions.label == predictions.prediction)\n",
        "top_wrd_df.show()\n",
        "top_wrd_df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
            "|               token|            features|label|       rawPrediction|         probability|prediction|\n",
            "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
            "| [ago, rock, become]|[0.54666000604629...|  0.0|[1.41761643875231...|[0.79741206092989...|       0.0|\n",
            "|   [ah, yeah, crazy]|[-1.4133000373840...|  1.0|[-0.1186564800670...|[0.24700552033410...|       1.0|\n",
            "|[already, young, ...|[0.13563999533653...|  0.0|[1.28606549779604...|[0.75176838242710...|       0.0|\n",
            "|[angel, style, shes]|[0.34995999932289...|  0.0|[1.67267610797735...|[0.82726193161393...|       0.0|\n",
            "| [angel, today, gun]|[0.34995999932289...|  0.0|[1.45533061587012...|[0.78274042762116...|       0.0|\n",
            "|[anymore, trouble...|[0.38657999038696...|  0.0|[1.94382655069015...|[0.85622207318998...|       0.0|\n",
            "|[anymore, trouble...|[0.38657999038696...|  0.0|[1.94382655069015...|[0.85622207318998...|       0.0|\n",
            "|    [baby, oh, need]|[0.49746999144554...|  0.0|[1.32920129829678...|[0.76951463006016...|       0.0|\n",
            "|[baby, without, d...|[0.49746999144554...|  0.0|[1.69711544022955...|[0.75172016807629...|       0.0|\n",
            "|[ball, shoulder, ...|[-0.0365669988095...|  0.0|[1.11691901131013...|[0.68746424103510...|       0.0|\n",
            "| [beast, break, son]|[0.01666799932718...|  0.0|[1.82487505980117...|[0.87246776270177...|       0.0|\n",
            "|   [beg, id, misery]|[-0.5893800258636...|  0.0|[1.28354985563475...|[0.64803839692784...|       0.0|\n",
            "|[believe, tell, lip]|[0.11221999675035...|  0.0|[1.41785560912943...|[0.75585586462292...|       0.0|\n",
            "|[belong, night, l...|[0.05777199938893...|  0.0|[1.84725264155818...|[0.86335479231514...|       0.0|\n",
            "|   [black, end, fun]|[-0.0574459992349...|  0.0|[1.44446352943006...|[0.75541221768381...|       0.0|\n",
            "|  [bleed, fool, sad]|[-0.4471800029277...|  0.0|[1.49521178959504...|[0.79979021610794...|       0.0|\n",
            "|[bleed, theyre, s...|[-0.4471800029277...|  0.0|[1.69556310572523...|[0.86402610677672...|       0.0|\n",
            "|   [blue, baby, man]|[0.08643099665641...|  0.0|[1.28216764117520...|[0.72858928539329...|       0.0|\n",
            "|[bottle, didnt, w...|[-0.5597000122070...|  0.0|[1.73357193432681...|[0.82902047276713...|       0.0|\n",
            "|[bout, pick, daug...|[1.11590003967285...|  0.0|[1.86913614352986...|[0.87784143494802...|       0.0|\n",
            "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "220"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4u5xK-up7L8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4940a147-af3c-4a21-c85f-bbd0c2eecd01"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, col\n",
        "\n",
        "window = Window.partitionBy(top_wrd_df['label']).orderBy(top_wrd_df['probability'].desc())\n",
        "\n",
        "top_wrd_df = top_wrd_df.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 10)\n",
        "top_wrd_df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RWwpxiftRkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "c9a229d3-4f06-4fb6-de91-dd46d79c7247"
      },
      "source": [
        "top_wrd_df_lst = top_wrd_df.groupBy('label').agg(F.collect_list(\"token\"))\n",
        "top_wrd_df_lst.show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|collect_list(token)                                                                                                                                                                                                          |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0.0  |[[human, mistake, watch], [put, fire, set], [spit, regret, round], [screen, yesterday, young], [stay, lot, till], [inside, deep, head], [fix, plain, thin], [black, fate, end], [circle, line, stone], [mirror, clear, away]]|\n",
            "|1.0  |[[oh, wasnt, cry], [love, hello, feelings], [oh, end, girl], [oh, care, wrong], [brother, foot, wanna], [oh, forget, remember], [chill, roll, like], [king, ring, weill], [bring, baby, gonna], [talk, bottle, yeah]]        |\n",
            "|2.0  |[[last, ah, yo]]                                                                                                                                                                                                             |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2UR533PbXDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainingSummary = lrModel.summary\n",
        "# roc = trainingSummary.roc.toPandas()\n",
        "# plt.plot(roc['FPR'],roc['TPR'])\n",
        "# plt.ylabel('False Positive Rate')\n",
        "# plt.xlabel('True Positive Rate')\n",
        "# plt.title('ROC Curve')\n",
        "# plt.show()\n",
        "# print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGBCO3WODyop",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJBCUrWuGDh9",
        "colab_type": "text"
      },
      "source": [
        "We can only use Naive Bayes for positive values, we have negative too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtpfiSjaD4Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyspark.ml.classification import NaiveBayes\n",
        "# nb = NaiveBayes(smoothing=1)\n",
        "# model = nb.fit(trainingData)\n",
        "# predictions = model.transform(testData)\n",
        "# # predictions.filter(predictions['prediction'] == 0) \\\n",
        "# #     .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
        "# #     .orderBy(\"probability\", ascending=False) \\\n",
        "# #     .show(n = 10, truncate = 30)\n",
        "# predictions.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaC8xG-oFewU",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g27o6p_mFhMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "algo = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
        "model = algo.fit(trainingData)\n",
        "predictions = model.transform(testData)\n",
        "predictions.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3XlC8QTG3tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "accracy = evaluator.evaluate(predictions)\n",
        "accracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfNX8oHhGrQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #important: need to cast to float type, and order by prediction, else it won't work\n",
        "# preds_and_labels = predictions.select(['prediction','d']).withColumn('label', F.col('d').cast(FloatType())).orderBy('prediction')\n",
        "\n",
        "# #select only prediction and label columns\n",
        "# preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
        "\n",
        "# metrics = MultiClassMetrics(preds_and_labels.rdd.map(tuple))\n",
        "\n",
        "y_true = predictions.select(['label']).collect()\n",
        "y_pred = predictions.select(['prediction']).collect()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K86GCzLAiN",
        "colab_type": "text"
      },
      "source": [
        "## Approach 2: Embeddings > Top 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIepLRQ1MMAa",
        "colab_type": "text"
      },
      "source": [
        "Start with creating embeddings first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtQQZkBYrkWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assembler = DocumentAssembler().setInputCol('text').setOutputCol('document')\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('token')#.setTargetPattern('/\\b(\\?You were looking)\\b/')#.setExceptionsPath('/content/MyDrive/SparkNLP/entities.txt')\n",
        "\n",
        "spell_chk = NorvigSweetingModel().pretrained().setInputCols(['token']).setOutputCol('corrected')\n",
        "\n",
        "lemmatizer = LemmatizerModel().pretrained().setInputCols(['corrected']).setOutputCol('lemma')\n",
        "\n",
        "normalizer = Normalizer().setInputCols(['lemma']).setOutputCol('normalized').setLowercase(True)\n",
        "\n",
        "stop_wrd = list( stopwords.words('english'))\n",
        "\n",
        "stop_words_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False).setStopWords(stop_wrd)\n",
        "\n",
        "embed = WordEmbeddingsModel.pretrained().setInputCols(['document',\"token\"]).setOutputCol(\"embeddings\")\n",
        "embeddings_finisher = EmbeddingsFinisher().setInputCols([\"embeddings\"]).setOutputCols([\"embeddings\"]).setOutputAsVector(True).setCleanAnnotations(False)\n",
        "\n",
        "finisher = Finisher().setInputCols(['cleanTokens']).setOutputCols(['cleanTokens']).setOutputAsArray(True).setCleanAnnotations(True)\n",
        "\n",
        "pipeline_embd = Pipeline().setStages([ assembler, tokenizer, spell_chk , lemmatizer, normalizer,stop_words_cleaner,embed,embeddings_finisher,finisher ]) \n",
        "model_trans_tfIdf_2 = pipeline_embd.fit(song_data) \n",
        "model_trans_tfIdf_2 = model_trans_tfIdf_2.transform(song_data) \n",
        "model_trans_tfIdf_2.show(10)\n",
        "# model_trans_tfIdf_2.select('embeddings').show(10,truncate = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF2UIViJQbU9",
        "colab_type": "text"
      },
      "source": [
        "1. Create TFIDF\n",
        "2. pick top 3 position where rank is max\n",
        "3. pick embeddings for that top 3 position\n",
        "4. map it to original words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs0YLwqiQeXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "\n",
        "count_vectorizer = CountVectorizer(inputCol='cleanTokens', outputCol='tf', minDF=10)\n",
        "cvmodel = count_vectorizer.fit(model_trans_tfIdf_2)\n",
        "idf = IDF(inputCol='tf', outputCol='features', minDocFreq=10)\n",
        "# finisher_idf = Finisher().setInputCols(['features']).setOutputCols(['tfidf']) finisher is for sparkNLP not spark ML\n",
        "bow_pipeline_2 = Pipeline().setStages([count_vectorizer, idf])\n",
        "bow_pipeline_2 = bow_pipeline_2.fit(model_trans_tfIdf_2)\n",
        "\n",
        "bows_2 = bow_pipeline_2.transform(model_trans_tfIdf_2)\n",
        "bows_2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8g1Ol6JTN2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "@udf(ArrayType(IntegerType()))\n",
        "def get_pos_top3(v):\n",
        "  #  if isinstance(v, SparseVector):\n",
        "    rank = v.values.tolist()\n",
        "    top_3_pos =sorted( sorted(range(len(rank)), key=lambda i: rank[i])[-3:])\n",
        "      # K = dict(zip(v.indices.tolist(), v.values.tolist()))\n",
        "      # top3 = sorted(K, key=K.get, reverse=True)[:3]\n",
        "    return top_3_pos\n",
        "  #  elif isinstance(v, DenseVector):\n",
        "  #     L =dict(zip(range(len(v)), v.values.tolist()))\n",
        "  #     top3 = sorted(L, key=L.get, reverse=True)[:3]\n",
        "  #     return top3 \n",
        "\n",
        "bows_wrdlevel_2 = bows_2.select(\"Key\",'Genre', get_pos_top3(\"features\").alias(\"top3_pos\"),'embeddings')\n",
        "bows_wrdlevel_2.na.drop(subset=[\"top3_pos\"])\n",
        "bows_wrdlevel_2.show()\n",
        "# bows_wrdlevel_2.select('embeddings').show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIiXVW-3scRz",
        "colab_type": "text"
      },
      "source": [
        "Select embeddings at those top 3 positions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKGpybZJsfk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.functions import struct, array\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "# @udf(ArrayType(ArrayType(FloatType())))\n",
        "# @udf(ArrayType(IntegerType()))\n",
        "# @udf(StringType())\n",
        "# @udf(IntegerType())\n",
        "@udf(VectorUDT())\n",
        "def get_embd_top3(v):\n",
        "  \n",
        "  # merged = list(itertools.chain(*embedd))\n",
        "  # merged = functools.reduce(operator.iconcat, embd_vec, [])\n",
        "  # mer_floats = [float(np_float) for np_float in merged]\n",
        "  # ver = Vectors.dense(mer_floats)\n",
        "  #create list of list from embedding using positions\n",
        "  indices = v[0]\n",
        "  embedd = v[1]\n",
        "  embd_top3 = []\n",
        "  \n",
        "  for i in indices:\n",
        "    sublist = [float(np_float) for np_float in embedd[i]]\n",
        "    embd_top3.append(sublist)\n",
        "  \n",
        "  #flatten out the list\n",
        "  merged = list(itertools.chain(*embd_top3))\n",
        "  \n",
        "  #convert to vector and send\n",
        "  ver = Vectors.dense(merged)\n",
        "\n",
        "  # tp = type(merged[0])\n",
        "  return ver\n",
        "  # return str(tp)\n",
        "  # return len(merged) \n",
        "\n",
        "bows_embed_2 = bows_wrdlevel_2.select(\"Key\",'Genre', get_embd_top3(struct('top3_pos', 'embeddings')).alias(\"features\"))\n",
        "bows_embed_2.na.drop(subset=[\"features\"])\n",
        "bows_embed_2.show(truncate = False)\n",
        "# bows_wrdlevel_2.select('embeddings').show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6wMl9Jcb5P9",
        "colab_type": "text"
      },
      "source": [
        "1. filter out column != 300\n",
        "2. stringlabel indexer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CxcmRrqb4u8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "import itertools\n",
        "import functools\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "# @udf(VectorUDT())\n",
        "@udf(IntegerType())\n",
        "def get_Flatvector_len_2(embd_vec):\n",
        "  # embd_vec = list(embd_vec)\n",
        "  # merged = list(itertools.chain(*embd_vec))\n",
        "  # merged = functools.reduce(operator.iconcat, embd_vec, [])\n",
        "  # mer_floats = [float(np_float) for np_float in merged]\n",
        "  vec =Vectors.dense(embd_vec)\n",
        "  return (len(vec))\n",
        "  # return str(type(mer_floats[0]))\n",
        "  # return embd_vec[0].tolist()\n",
        "\n",
        "dataset_clsf_300_2 = bows_embed_2.select('features',get_Flatvector_len_2(F.col('features')).alias('feat_cnt'),'Genre')\n",
        "dataset_clsf_300_2.na.drop(subset=[\"feat_cnt\"])\n",
        "dataset_clsf_300_2.na.drop(subset=[\"features\"])\n",
        "# dataset_clsf_300.na.drop(subset=[\"label\"])\n",
        "# dataset_clsf.show(truncate = True)\n",
        "# dataset_clsf = dataset_clsf.groupBy(\"features\").agg(F.count('features'))\n",
        "dataset_clsf_300_2 = dataset_clsf_300_2.filter((dataset_clsf_300_2.feat_cnt == 300) | (dataset_clsf_300_2.Genre != ' ') ).select('features','Genre')\n",
        "# dataset_clsf_300 = dataset_clsf_300.filter(dataset_clsf_300.label != ' ').select('features','label')\n",
        "# dataset_clsf_300 = dataset_clsf_300.filter(F.col('features') != ' ')\n",
        "# dataset_clsf_300.show(dataset_clsf_300.count() )\n",
        "# dataset_clsf_300.repartition(1).count()\n",
        "\n",
        "dataset_clsf_300_2.show()\n",
        "# dataset_clsf_300.groupBy(\"feat_cnt\").agg(F.count('feat_cnt')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-FW5vXljN77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"Genre\", outputCol = \"label\" ,)\n",
        "pipeline = Pipeline(stages=[ label_stringIdx])\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(dataset_clsf_300_2)\n",
        "dataset = pipelineFit.transform(dataset_clsf_300_2)\n",
        "selectedCols = ['features','label']\n",
        "dataset = dataset.select(selectedCols)\n",
        "dataset.show(5)\n",
        "dataset.select('label').distinct().collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5RBPtrkaMp",
        "colab_type": "text"
      },
      "source": [
        "continue classification from here............................."
      ]
    }
  ]
}