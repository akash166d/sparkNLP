{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_sentiment_eg_SparkNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMfn8Wc5ngxA5dd4SzprGLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash166d/sparkNLP/blob/master/NLP_sentiment_eg_SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBmc0480QLzB",
        "colab_type": "text"
      },
      "source": [
        "# Install Package/Lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwc2OqTH16VJ",
        "colab_type": "text"
      },
      "source": [
        "This is required to ensure the ubuntu related dependencies are up to date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQwcI3O3KCty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c145638d-1f25-4fda-ac47-ef67baa9e888"
      },
      "source": [
        "! sudo apt-get update --fix-missing\n",
        "! sudo apt-get upgrade --fix-missing"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [1 In\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Wait\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [4 Release 697 B/697 B 100%] [Wa\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers] [Waitin\r                                                                               \rHit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,038 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,856 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [116 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,413 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,558 B]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [882 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,334 kB]\n",
            "Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [895 kB]\n",
            "Fetched 8,099 kB in 5s (1,531 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following packages have been kept back:\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "  r-cran-tidyr\n",
            "The following packages will be upgraded:\n",
            "  base-files binutils binutils-common binutils-x86-64-linux-gnu bsdutils\n",
            "  cuda-compat-10-1 e2fsprogs fdisk kmod libbinutils libblkid1 libc-bin\n",
            "  libcom-err2 libext2fs2 libfdisk1 libgcrypt20 libgnutls30 libkmod2\n",
            "  libldap-2.4-2 libldap-common libmount1 libnss3 libpam-systemd libpulse0\n",
            "  libsasl2-2 libsasl2-modules-db libseccomp2 libsmartcols1 libsqlite3-0 libss2\n",
            "  libssh-gcrypt-4 libsystemd0 libudev1 linux-libc-dev module-init-tools mount\n",
            "  openssl python3-software-properties r-cran-dplyr r-cran-dt r-cran-fs\n",
            "  r-cran-ps software-properties-common systemd systemd-sysv udev util-linux\n",
            "47 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 23.1 MB of archives.\n",
            "After this operation, 315 kB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-compat-10-1 418.152.00-1 [5,246 kB]\n",
            "Get:2 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-dplyr amd64 1.0.1-1cran1.1804.0 [1,040 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 base-files amd64 10.1ubuntu2.9 [59.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 bsdutils amd64 1:2.31.1-0.4ubuntu3.6 [60.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libext2fs2 amd64 1.44.1-1ubuntu1.3 [157 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 e2fsprogs amd64 1.44.1-1ubuntu1.3 [391 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libblkid1 amd64 2.31.1-0.4ubuntu3.6 [124 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libfdisk1 amd64 2.31.1-0.4ubuntu3.6 [164 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmount1 amd64 2.31.1-0.4ubuntu3.6 [136 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsmartcols1 amd64 2.31.1-0.4ubuntu3.6 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 fdisk amd64 2.31.1-0.4ubuntu3.6 [108 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 util-linux amd64 2.31.1-0.4ubuntu3.6 [903 kB]\n",
            "Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-dt all 0.15-1cran1.1804.0 [1,199 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-bin amd64 2.27-3ubuntu1.2 [637 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 systemd-sysv amd64 237-3ubuntu10.42 [15.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam-systemd amd64 237-3ubuntu10.42 [107 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsystemd0 amd64 237-3ubuntu10.42 [208 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 systemd amd64 237-3ubuntu10.42 [2,914 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 udev amd64 237-3ubuntu10.42 [1,102 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev1 amd64 237-3ubuntu10.42 [57.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 kmod amd64 24-1ubuntu3.5 [88.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkmod2 amd64 24-1ubuntu3.5 [40.2 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mount amd64 2.31.1-0.4ubuntu3.6 [107 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcom-err2 amd64 1.44.1-1ubuntu1.3 [8,848 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgcrypt20 amd64 1.8.1-4ubuntu1.2 [417 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libss2 amd64 1.44.1-1ubuntu1.3 [11.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgnutls30 amd64 3.5.18-1ubuntu1.4 [645 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libseccomp2 amd64 2.4.3-1ubuntu3.18.04.3 [42.0 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsqlite3-0 amd64 3.22.0-1ubuntu0.4 [499 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssl amd64 1.1.1-1ubuntu2.1~18.04.6 [614 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.4 [1,839 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.4 [196 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.4 [3,392 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.4 [488 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.1 [14.8 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.1 [49.2 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.6 [17.0 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.6 [155 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnss3 amd64 2:3.35-2ubuntu2.11 [1,221 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse0 amd64 1:11.1-1ubuntu7.10 [266 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libssh-gcrypt-4 amd64 0.8.0~20170825.94fa1e38-1ubuntu0.7 [172 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-112.113 [982 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 module-init-tools all 24-1ubuntu3.5 [2,516 B]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 software-properties-common all 0.96.24.32.14 [10.1 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-software-properties all 0.96.24.32.14 [23.9 kB]\n",
            "Get:46 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-fs amd64 1.5.0-1cran1.1804.0 [254 kB]\n",
            "Get:47 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-ps amd64 1.3.4-1cran1.1804.0 [206 kB]\n",
            "Fetched 23.1 MB in 7s (3,195 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 47.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_10.1ubuntu2.9_amd64.deb ...\n",
            "Unpacking base-files (10.1ubuntu2.9) over (10.1ubuntu2.7) ...\n",
            "Setting up base-files (10.1ubuntu2.9) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../bsdutils_1%3a2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking bsdutils (1:2.31.1-0.4ubuntu3.6) over (1:2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up bsdutils (1:2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libext2fs2_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libext2fs2:amd64 (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up libext2fs2:amd64 (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../e2fsprogs_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking e2fsprogs (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up e2fsprogs (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libblkid1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libblkid1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libblkid1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libfdisk1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libfdisk1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libfdisk1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libmount1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libmount1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libmount1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libsmartcols1_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking libsmartcols1:amd64 (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up libsmartcols1:amd64 (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../fdisk_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking fdisk (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up fdisk (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../util-linux_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking util-linux (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Setting up util-linux (2.31.1-0.4ubuntu3.6) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-bin_2.27-3ubuntu1.2_amd64.deb ...\n",
            "Unpacking libc-bin (2.27-3ubuntu1.2) over (2.27-3ubuntu1) ...\n",
            "Setting up libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../systemd-sysv_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking systemd-sysv (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../libpam-systemd_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking libpam-systemd:amd64 (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../libsystemd0_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking libsystemd0:amd64 (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Setting up libsystemd0:amd64 (237-3ubuntu10.42) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../systemd_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking systemd (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../udev_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking udev (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Preparing to unpack .../libudev1_237-3ubuntu10.42_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (237-3ubuntu10.42) over (237-3ubuntu10.41) ...\n",
            "Setting up libudev1:amd64 (237-3ubuntu10.42) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../kmod_24-1ubuntu3.5_amd64.deb ...\n",
            "Unpacking kmod (24-1ubuntu3.5) over (24-1ubuntu3.4) ...\n",
            "Preparing to unpack .../libkmod2_24-1ubuntu3.5_amd64.deb ...\n",
            "Unpacking libkmod2:amd64 (24-1ubuntu3.5) over (24-1ubuntu3.4) ...\n",
            "Preparing to unpack .../mount_2.31.1-0.4ubuntu3.6_amd64.deb ...\n",
            "Unpacking mount (2.31.1-0.4ubuntu3.6) over (2.31.1-0.4ubuntu3.4) ...\n",
            "Preparing to unpack .../libcom-err2_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libcom-err2:amd64 (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up libcom-err2:amd64 (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libgcrypt20_1.8.1-4ubuntu1.2_amd64.deb ...\n",
            "Unpacking libgcrypt20:amd64 (1.8.1-4ubuntu1.2) over (1.8.1-4ubuntu1.1) ...\n",
            "Setting up libgcrypt20:amd64 (1.8.1-4ubuntu1.2) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libss2_1.44.1-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libss2:amd64 (1.44.1-1ubuntu1.3) over (1.44.1-1ubuntu1.2) ...\n",
            "Setting up libss2:amd64 (1.44.1-1ubuntu1.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.5.18-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.5.18-1ubuntu1.4) over (3.5.18-1ubuntu1.1) ...\n",
            "Setting up libgnutls30:amd64 (3.5.18-1ubuntu1.4) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libseccomp2_2.4.3-1ubuntu3.18.04.3_amd64.deb ...\n",
            "Unpacking libseccomp2:amd64 (2.4.3-1ubuntu3.18.04.3) over (2.4.1-0ubuntu0.18.04.2) ...\n",
            "Setting up libseccomp2:amd64 (2.4.3-1ubuntu3.18.04.3) ...\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libsqlite3-0_3.22.0-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking libsqlite3-0:amd64 (3.22.0-1ubuntu0.4) over (3.22.0-1ubuntu0.1) ...\n",
            "Preparing to unpack .../01-openssl_1.1.1-1ubuntu2.1~18.04.6_amd64.deb ...\n",
            "Unpacking openssl (1.1.1-1ubuntu2.1~18.04.6) over (1.1.1-1ubuntu2.1~18.04.5) ...\n",
            "Preparing to unpack .../02-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../03-binutils-common_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../04-binutils_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking binutils (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../05-libbinutils_2.30-21ubuntu1~18.04.4_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.4) over (2.30-21ubuntu1~18.04.2) ...\n",
            "Preparing to unpack .../06-cuda-compat-10-1_418.152.00-1_amd64.deb ...\n",
            "Unpacking cuda-compat-10-1 (418.152.00-1) over (418.87.01-1) ...\n",
            "Preparing to unpack .../07-libsasl2-modules-db_2.1.27~101-g0780600+dfsg-3ubuntu2.1_amd64.deb ...\n",
            "Unpacking libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) over (2.1.27~101-g0780600+dfsg-3ubuntu2) ...\n",
            "Preparing to unpack .../08-libsasl2-2_2.1.27~101-g0780600+dfsg-3ubuntu2.1_amd64.deb ...\n",
            "Unpacking libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) over (2.1.27~101-g0780600+dfsg-3ubuntu2) ...\n",
            "Preparing to unpack .../09-libldap-common_2.4.45+dfsg-1ubuntu1.6_all.deb ...\n",
            "Unpacking libldap-common (2.4.45+dfsg-1ubuntu1.6) over (2.4.45+dfsg-1ubuntu1.4) ...\n",
            "Preparing to unpack .../10-libldap-2.4-2_2.4.45+dfsg-1ubuntu1.6_amd64.deb ...\n",
            "Unpacking libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.6) over (2.4.45+dfsg-1ubuntu1.4) ...\n",
            "Preparing to unpack .../11-libnss3_2%3a3.35-2ubuntu2.11_amd64.deb ...\n",
            "Unpacking libnss3:amd64 (2:3.35-2ubuntu2.11) over (2:3.35-2ubuntu2.9) ...\n",
            "Preparing to unpack .../12-libpulse0_1%3a11.1-1ubuntu7.10_amd64.deb ...\n",
            "Unpacking libpulse0:amd64 (1:11.1-1ubuntu7.10) over (1:11.1-1ubuntu7.9) ...\n",
            "Preparing to unpack .../13-libssh-gcrypt-4_0.8.0~20170825.94fa1e38-1ubuntu0.7_amd64.deb ...\n",
            "Unpacking libssh-gcrypt-4:amd64 (0.8.0~20170825.94fa1e38-1ubuntu0.7) over (0.8.0~20170825.94fa1e38-1ubuntu0.6) ...\n",
            "Preparing to unpack .../14-linux-libc-dev_4.15.0-112.113_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-112.113) over (4.15.0-72.81) ...\n",
            "Preparing to unpack .../15-module-init-tools_24-1ubuntu3.5_all.deb ...\n",
            "Unpacking module-init-tools (24-1ubuntu3.5) over (24-1ubuntu3.4) ...\n",
            "Preparing to unpack .../16-software-properties-common_0.96.24.32.14_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.32.14) over (0.96.24.32.13) ...\n",
            "Preparing to unpack .../17-python3-software-properties_0.96.24.32.14_all.deb ...\n",
            "Unpacking python3-software-properties (0.96.24.32.14) over (0.96.24.32.13) ...\n",
            "Preparing to unpack .../18-r-cran-dplyr_1.0.1-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-dplyr (1.0.1-1cran1.1804.0) over (1.0.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../19-r-cran-dt_0.15-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-dt (0.15-1cran1.1804.0) over (0.14-1cran1.1804.0) ...\n",
            "Preparing to unpack .../20-r-cran-fs_1.5.0-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-fs (1.5.0-1cran1.1804.0) over (1.4.2-1cran1.1804.0) ...\n",
            "Preparing to unpack .../21-r-cran-ps_1.3.4-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-ps (1.3.4-1cran1.1804.0) over (1.3.3-1cran1.1804.0) ...\n",
            "Setting up r-cran-ps (1.3.4-1cran1.1804.0) ...\n",
            "Setting up libldap-common (2.4.45+dfsg-1ubuntu1.6) ...\n",
            "Setting up cuda-compat-10-1 (418.152.00-1) ...\n",
            "Setting up libssh-gcrypt-4:amd64 (0.8.0~20170825.94fa1e38-1ubuntu0.7) ...\n",
            "Setting up libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-112.113) ...\n",
            "Setting up mount (2.31.1-0.4ubuntu3.6) ...\n",
            "Setting up libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Setting up libkmod2:amd64 (24-1ubuntu3.5) ...\n",
            "Setting up libpulse0:amd64 (1:11.1-1ubuntu7.10) ...\n",
            "Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.4) ...\n",
            "Setting up r-cran-dt (0.15-1cran1.1804.0) ...\n",
            "Setting up udev (237-3ubuntu10.42) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Setting up libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.6) ...\n",
            "Setting up r-cran-fs (1.5.0-1cran1.1804.0) ...\n",
            "Setting up systemd (237-3ubuntu10.42) ...\n",
            "Setting up r-cran-dplyr (1.0.1-1cran1.1804.0) ...\n",
            "Setting up openssl (1.1.1-1ubuntu2.1~18.04.6) ...\n",
            "Setting up libsqlite3-0:amd64 (3.22.0-1ubuntu0.4) ...\n",
            "Setting up python3-software-properties (0.96.24.32.14) ...\n",
            "Setting up software-properties-common (0.96.24.32.14) ...\n",
            "Setting up kmod (24-1ubuntu3.5) ...\n",
            "Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.4) ...\n",
            "Setting up systemd-sysv (237-3ubuntu10.42) ...\n",
            "Setting up libnss3:amd64 (2:3.35-2ubuntu2.11) ...\n",
            "Setting up module-init-tools (24-1ubuntu3.5) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.4) ...\n",
            "Setting up libpam-systemd:amd64 (237-3ubuntu10.42) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up binutils (2.30-21ubuntu1~18.04.4) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q2etGjaMgCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "d9567b5f-13b8-4474-800d-100dbcfa530c"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed -q pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed -q spark-nlp==2.5.4\n",
        "\n",
        "! pip install --user -U nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_265\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 62kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 45.0MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.6MB/s \n",
            "\u001b[?25hCollecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434677 sha256=4adac7b38a12c7437ab362b16b07da218c93cf1a875eace61a420e43cfb5b610\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2RzyRTnNGjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "830bb514-f2f4-4c3c-da94-6caa63b681df"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version; \", spark.version)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version:  2.5.4\n",
            "Apache Spark version;  2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0yrTMJaQWqz",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive and read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD74i7zONK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "dc71c811-b73f-409d-bb55-44051745ec65"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS0s8wYL25E9",
        "colab_type": "text"
      },
      "source": [
        "Some of the sparkNLP annotators were having trouble reading spaces in path, hence used symbolic link to replace the path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSDHjKbOmpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ln -s \"/content/drive/My Drive\" \"/content/MyDrive\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7sjN-NzO1tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "song_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/content/drive/My Drive/SparkNLP/song_2k.csv\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p--pxgIPSQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "63845394-71ba-420c-a947-9956f2cf0ed2"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "song_data = song_data.select(['Key', 'Lyric', 'Genre']).withColumnRenamed('Lyric','text')\n",
        "\n",
        "# train, test = trainDataset.randomSplit(weights=[0.5, 0.5], seed=123)\n",
        "song_data = song_data.limit(999)\n",
        "print(song_data.count())\n",
        "\n",
        "song_data = song_data.filter(song_data.text != '')\n",
        "print(song_data.count())\n",
        "song_data = song_data.filter((song_data.Genre == 'Rock') | (song_data.Genre == 'Hip Hop') | (song_data.Genre == 'Pop')  )\n",
        "# song_data = song_data.filter(song_data.Genre != '')\n",
        "print(song_data.count())\n",
        "# song_data = song_data.filter(song_data.text != ' ')\n",
        "# print(song_data.count())\n",
        "# song_data = song_data.filter(song_data.Genre != ' ')\n",
        "# print(song_data.count())\n",
        "song_data.na.drop(subset=[\"text\"])\n",
        "print(song_data.count())\n",
        "song_data.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n",
            "999\n",
            "915\n",
            "915\n",
            "+--------------------+--------------------+-------+\n",
            "|                 Key|                text|  Genre|\n",
            "+--------------------+--------------------+-------+\n",
            "|10000 maniacs_Mor...|I could feel at t...|   Rock|\n",
            "|10000 maniacs_Bec...|Take me now, baby...|   Rock|\n",
            "|jamiroquai_Rock D...|And it's coming a...|    Pop|\n",
            "|10000 maniacs_The...|These are. These ...|   Rock|\n",
            "|10000 maniacs_Eve...|Trudging slowly o...|   Rock|\n",
            "|10000 maniacs_Don...|Don't talk, I wil...|   Rock|\n",
            "|black veil brides...|Have we begun to ...|   Rock|\n",
            "|lynyrd skynyrd_I ...|Ain't no need to ...|   Rock|\n",
            "|10000 maniacs_Acr...|Well they left th...|   Rock|\n",
            "|10000 maniacs_Pla...|[ music: Dennis D...|   Rock|\n",
            "|10000 maniacs_Rai...|On bended kneeI'v...|   Rock|\n",
            "|twista_Back 2 School|[Tung Twista]. ba...|Hip Hop|\n",
            "|10000 maniacs_Ant...|For whom do the b...|   Rock|\n",
            "|10000 maniacs_All...|She walks alone o...|   Rock|\n",
            "|10000 maniacs_Bac...|Jenny. Jenny you ...|   Rock|\n",
            "|cyndi lauper_True...|You with the sad ...|    Pop|\n",
            "|10000 maniacs_A R...|You were looking ...|   Rock|\n",
            "|rick astley_She M...|She makes me more...|    Pop|\n",
            "|steve earle_Nothi...|I'm the keeper of...|   Rock|\n",
            "|10000 maniacs_Mad...|\"the legs of Madd...|   Rock|\n",
            "+--------------------+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyXMOyWZP4Xb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b4f395f-9245-4b29-be01-31ba418e6368"
      },
      "source": [
        "print(type(song_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPdwWmc3iPz",
        "colab_type": "text"
      },
      "source": [
        "# Package Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeBx3-P6P6Q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a65de563-6d1e-4fb2-a756-6bd5b5e238c4"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from pyspark.sql import functions as F"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_L4yv9Qc_-",
        "colab_type": "text"
      },
      "source": [
        "# Vocab Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiHyoUe_33ci",
        "colab_type": "text"
      },
      "source": [
        "Basic Data cleaning for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atU2csZwRIJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "bb0225b4-1679-416a-9ea9-2c3d77c43f94"
      },
      "source": [
        "assembler = DocumentAssembler().setInputCol('text').setOutputCol('document')\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('token')#.setTargetPattern('/\\b(\\?You were looking)\\b/')#.setExceptionsPath('/content/MyDrive/SparkNLP/entities.txt')\n",
        "\n",
        "spell_chk = NorvigSweetingModel().pretrained().setInputCols(['token']).setOutputCol('corrected')\n",
        "\n",
        "lemmatizer = LemmatizerModel().pretrained().setInputCols(['corrected']).setOutputCol('lemma')\n",
        "\n",
        "normalizer = Normalizer().setInputCols(['lemma']).setOutputCol('normalized').setLowercase(True)\n",
        "\n",
        "stop_wrd = list( stopwords.words('english'))\n",
        "\n",
        "stop_words_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False).setStopWords(stop_wrd)\n",
        "\n",
        "# token_assembler = TokenAssembler().setInputCols([\"cleanTokens\"]).setOutputCol(\"assembled\")\n",
        "\n",
        "finisher = Finisher().setInputCols(['cleanTokens']).setOutputCols(['cleanTokens']).setOutputAsArray(True)#.setValueSplitSymbol(\" \")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spellcheck_norvig download started this may take some time.\n",
            "Approximate size to download 4.2 MB\n",
            "[OK!]\n",
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfOlqRGP4qnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "bccd6047-7cef-4843-d522-7a532b2614f5"
      },
      "source": [
        "pipeline_bow = Pipeline().setStages([\n",
        "    assembler, tokenizer, spell_chk , \n",
        "    lemmatizer, normalizer,stop_words_cleaner,finisher\n",
        "])\n",
        "model_trans_tfIdf = pipeline_bow.fit(song_data)\n",
        "model_trans_tfIdf =  model_trans_tfIdf.transform(song_data)\n",
        "model_trans_tfIdf.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------+--------------------+\n",
            "|                 Key|                text|  Genre|         cleanTokens|\n",
            "+--------------------+--------------------+-------+--------------------+\n",
            "|10000 maniacs_Mor...|I could feel at t...|   Rock|[could, feel, tim...|\n",
            "|10000 maniacs_Bec...|Take me now, baby...|   Rock|[take, baby, hold...|\n",
            "|jamiroquai_Rock D...|And it's coming a...|    Pop|[come, baby, yeah...|\n",
            "|10000 maniacs_The...|These are. These ...|   Rock|[day, youll, reme...|\n",
            "|10000 maniacs_Eve...|Trudging slowly o...|   Rock|[grudge, slowly, ...|\n",
            "|10000 maniacs_Don...|Don't talk, I wil...|   Rock|[donut, talk, lis...|\n",
            "|black veil brides...|Have we begun to ...|   Rock|[begin, drift, aw...|\n",
            "|lynyrd skynyrd_I ...|Ain't no need to ...|   Rock|[aint, need, worr...|\n",
            "|10000 maniacs_Acr...|Well they left th...|   Rock|[well, leave, mor...|\n",
            "|10000 maniacs_Pla...|[ music: Dennis D...|   Rock|[music, dennis, d...|\n",
            "|10000 maniacs_Rai...|On bended kneeI'v...|   Rock|[bend, kneeive, l...|\n",
            "|twista_Back 2 School|[Tung Twista]. ba...|Hip Hop|[tung, twista, ba...|\n",
            "|10000 maniacs_Ant...|For whom do the b...|   Rock|[bell, toll, sent...|\n",
            "|10000 maniacs_All...|She walks alone o...|   Rock|[walk, alone, bri...|\n",
            "|10000 maniacs_Bac...|Jenny. Jenny you ...|   Rock|[jenny, jenny, do...|\n",
            "|cyndi lauper_True...|You with the sad ...|    Pop|[sad, eye, donut,...|\n",
            "|10000 maniacs_A R...|You were looking ...|   Rock|[look, away, west...|\n",
            "|rick astley_She M...|She makes me more...|    Pop|[make, could, eve...|\n",
            "|steve earle_Nothi...|I'm the keeper of...|   Rock|[im, keeper, hear...|\n",
            "|10000 maniacs_Mad...|\"the legs of Madd...|   Rock|[leg, maddox, kit...|\n",
            "+--------------------+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWrdAicAzKvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "d8eb35ae-2ca9-4e3f-8546-857db4012a80"
      },
      "source": [
        "#convert list to sentence\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.types import *\n",
        "@udf( StringType())\n",
        "def get_sentence(word_in):\n",
        "    return \" \".join(str(item) for item in word_in)\n",
        "\n",
        "# udf_to_doc = udf(get_sentence, StringType())\n",
        "\n",
        "df_cleanToken = model_trans_tfIdf.select('Key','Genre', get_sentence(F.col('cleanTokens')).alias('text'))\n",
        "# bow_top_embd = bow_top_embd.filter((F.col('doc') != '') | F.col('doc') != ' ')\n",
        "df_cleanToken.na.drop(subset=[\"text\"])\n",
        "from pyspark.sql.functions import trim\n",
        "df_cleanToken = df_cleanToken.withColumn(\"text\", trim(df_cleanToken.text))\n",
        "# bow_top_embd.filter(!(F.col('doc') == ' ')).show()\n",
        "df_cleanToken.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+--------------------+\n",
            "|                 Key|  Genre|                text|\n",
            "+--------------------+-------+--------------------+\n",
            "|10000 maniacs_Mor...|   Rock|could feel time w...|\n",
            "|10000 maniacs_Bec...|   Rock|take baby hold cl...|\n",
            "|jamiroquai_Rock D...|    Pop|come baby yeah co...|\n",
            "|10000 maniacs_The...|   Rock|day youll remembe...|\n",
            "|10000 maniacs_Eve...|   Rock|grudge slowly wet...|\n",
            "|10000 maniacs_Don...|   Rock|donut talk listen...|\n",
            "|black veil brides...|   Rock|begin drift away ...|\n",
            "|lynyrd skynyrd_I ...|   Rock|aint need worry a...|\n",
            "|10000 maniacs_Acr...|   Rock|well leave mornin...|\n",
            "|10000 maniacs_Pla...|   Rock|music dennis drew...|\n",
            "|10000 maniacs_Rai...|   Rock|bend kneeive look...|\n",
            "|twista_Back 2 School|Hip Hop|tung twista back ...|\n",
            "|10000 maniacs_Ant...|   Rock|bell toll sentenc...|\n",
            "|10000 maniacs_All...|   Rock|walk alone brick ...|\n",
            "|10000 maniacs_Bac...|   Rock|jenny jenny dont ...|\n",
            "|cyndi lauper_True...|    Pop|sad eye donut dis...|\n",
            "|10000 maniacs_A R...|   Rock|look away western...|\n",
            "|rick astley_She M...|    Pop|make could ever m...|\n",
            "|steve earle_Nothi...|   Rock|im keeper heart k...|\n",
            "|10000 maniacs_Mad...|   Rock|leg maddox kitche...|\n",
            "+--------------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-KZPyPADWga",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8fhOArDaHZ",
        "colab_type": "text"
      },
      "source": [
        "## Analyze_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0-Acu2Yv9fV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "37f20591-1be6-473c-aed2-2099d1565fa2"
      },
      "source": [
        "pipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n",
        "# mod_sent = pipeline.fit(model_trans_tfIdf)\n",
        "sent_df = pipeline.transform(df_cleanToken)\n",
        "sent_df.select('sentiment').show(truncate = False)\n",
        "sent_df.select('sentiment')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n",
            "+------------------------------------------------------------+\n",
            "|sentiment                                                   |\n",
            "+------------------------------------------------------------+\n",
            "|[[sentiment, 0, 343, positive, [confidence -> 0.6221], []]] |\n",
            "|[[sentiment, 0, 663, positive, [confidence -> 0.8311], []]] |\n",
            "|[[sentiment, 0, 1008, positive, [confidence -> 0.8024], []]]|\n",
            "|[[sentiment, 0, 417, positive, [confidence -> 0.5757], []]] |\n",
            "|[[sentiment, 0, 413, positive, [confidence -> 0.9414], []]] |\n",
            "|[[sentiment, 0, 716, negative, [confidence -> 0.4813], []]] |\n",
            "|[[sentiment, 0, 1014, positive, [confidence -> 0.7405], []]]|\n",
            "|[[sentiment, 0, 425, positive, [confidence -> 0.6867], []]] |\n",
            "|[[sentiment, 0, 322, positive, [confidence -> 0.9522], []]] |\n",
            "|[[sentiment, 0, 571, positive, [confidence -> 0.7857], []]] |\n",
            "|[[sentiment, 0, 379, positive, [confidence -> 0.8943], []]] |\n",
            "|[[sentiment, 0, 1876, positive, [confidence -> 0.7895], []]]|\n",
            "|[[sentiment, 0, 588, positive, [confidence -> 0.6725], []]] |\n",
            "|[[sentiment, 0, 556, positive, [confidence -> 0.7489], []]] |\n",
            "|[[sentiment, 0, 1140, positive, [confidence -> 0.5447], []]]|\n",
            "|[[sentiment, 0, 721, positive, [confidence -> 0.8772], []]] |\n",
            "|[[sentiment, 0, 346, positive, [confidence -> 0.4822], []]] |\n",
            "|[[sentiment, 0, 651, positive, [confidence -> 0.8102], []]] |\n",
            "|[[sentiment, 0, 334, negative, [confidence -> 0.8324], []]] |\n",
            "|[[sentiment, 0, 909, positive, [confidence -> 0.5447], []]] |\n",
            "+------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[sentiment: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgweVZdZ9QP0",
        "colab_type": "text"
      },
      "source": [
        "Save sentiment result and corresponding confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP5o6sSgB7GP",
        "colab_type": "text"
      },
      "source": [
        "1. find result and confidence column\n",
        "2. find top rows based on confidence and find top words for them using TF-IDF rank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lRgGDa59T86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42b8d9e8-3593-434b-fb41-64e377f60b37"
      },
      "source": [
        "#convert list to sentence\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.types import *\n",
        "@udf(StringType())\n",
        "# @udf(IntegerType())\n",
        "def get_result_conf(sent):\n",
        "    result = sentiment[2]\n",
        "    tp = type(result)\n",
        "    # len = len(sent[0])\n",
        "    return str(tp)\n",
        "    # return int(len)\n",
        "    # return \" \".join(str(item) for item in word_in)\n",
        "\n",
        "# udf_to_doc = udf(get_sentence, StringType())\n",
        "\n",
        "df_sent_result = sent_df.select('Key', get_result_conf(F.col('sentiment')).alias('result'))\n",
        "# bow_top_embd = bow_top_embd.filter((F.col('doc') != '') | F.col('doc') != ' ')\n",
        "df_sent_result.na.drop(subset=[\"result\"])\n",
        "# from pyspark.sql.functions import trim\n",
        "# df_cleanToken = df_cleanToken.withColumn(\"text\", trim(df_cleanToken.text))\n",
        "# bow_top_embd.filter(!(F.col('doc') == ' ')).show()\n",
        "df_sent_result.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-459d26d955f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# df_cleanToken = df_cleanToken.withColumn(\"text\", trim(df_cleanToken.text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# bow_top_embd.filter(!(F.col('doc') == ' ')).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_sent_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8504.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 140, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-53-459d26d955f3>\", line 7, in get_result_conf\nNameError: name 'sentiment' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor115.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-53-459d26d955f3>\", line 7, in get_result_conf\nNameError: name 'sentiment' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    }
  ]
}